---
title: "Curvature measures from index constrained eigenvalue samples"
author: "Dennis Amelunxen"
date: "12 December, 2017"
output: rmarkdown::html_vignette
bibliography: references.bib
link-citations: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "curv-meas-from-constr-evals_figures/"
)
```

```{r load-pkgs, include=FALSE}
library(symconivol)
library(tidyverse)
library(knitr)
library(png)
library(Rmisc)
library(rstan)
```

This note analyzes the curvature measures of symmetric cones
(positive semdefinite real symmetric/complex unitary/quaternion unitary matrices)
through the distribution of the Gaussian orthogonal/unitary/symplectic ensemble
conditioned on the index function, that is, the number of positive eigenvalues.
The approach follows closely the approach of reconstructing the conic intrinsic
volumes from the corresponding bivariate chi-bar-squared distribution.
We will assume familiarity with this approach, which is explained in the 
vignette [Estimating conic intrinsic volumes from bivariate chi-bar-squared data](../../conivol/vignettes/estim-conic-intrinsic-volumes-with-EM.html)
from the `conivol` package.
We present the connection between the curvature measures and the index constrained
Gaussian orthogonal/unitary/symplectic ensemble without proof; for more information
and references, see [@AB15s].


**Vignettes from the `conivol` package:**

* [Conic intrinsic volumes and (bivariate) chi-bar-squared distribution](../../conivol/vignettes/conic-intrinsic-volumes.html):
    introduces conic intrinsic volumes and (bivariate) chi-bar-squared distributions,
    as well as the computations involving polyhedral cones,
* [Estimating conic intrinsic volumes from bivariate chi-bar-squared data](../../conivol/vignettes/estim-conic-intrinsic-volumes-with-EM.html):
    describes the details of the algorithm for finding the intrinsic volumes of closed
    convex cones from samples of the associated bivariate chi-bar-squared distribution,
* [Bayesian estimates for conic intrinsic volumes](../../conivol/vignettes/bayesian.html):
    describes the Bayesian approach for reconstructing intrinsic volumes
    from sampling data, which can either be samples from the intrinsic
    volumes distribution (in the case of polyhedral cones), or from the
    bivariate chi-bar-squared distribution, and which can be with or without
    enforcing log-concavity of the intrinsic volumes.


## Symmetric cones

[Symmetric cones](https://en.wikipedia.org/wiki/Symmetric_cone) are self-dual convex cones
with a transitive group of symmetries. Every symmetric cone decomposes into an
orthogonal sum of a finite number of simple symmetric cones, which consist of

* the Lorentz cones (circular cones of radius $\pi/4$),
* positive semdefinite real symmetric matrices,
* positive semdefinite complex unitary matrices,
* positive semdefinite quaternion unitary matrices,
* positive semidefinite octionion unitary $3\times 3$-matrices.

The first four points each describe families of cones, while the last point is just
one exceptional cone. The Lorentz cones have a very simple structure and do not
have the kind of structure that we focus on in this note. We also do not discuss the
exceptional symmertic cone. So "symmetric cone" in this note is synonymous to a cone
of positive semidefinite real symmetric/complex unitary/quaternion unitary matrices
of a certain format.

We use the Dyson index $\beta\in\{1,2,4\}$ to indicate whether the ground field is
real ($\beta=1$), complex ($\beta=2$), or quaternion ($\beta=4$); we denote
the size of the matrices by $n$. The set of real symmetric/complex unitary/quaternion
unitary matrices form a Euclidean space $\mathcal{E}$ of dimension
    \[ d = n + \beta\binom{n}{2}
        = \frac{\beta}{2} n^2 \cdot \begin{cases} (1+\frac{1}{n}) & \text{if } \beta=1
            \\ 1 & \text{if } \beta=2
            \\ (1-\frac{1}{2n}) & \text{if } \beta=4 .
            \end{cases} \]
The Gaussian orthogonal/symmetric/unitary ensemble, which we denote by
$\text{G}\beta\text{E}$, is the standard normal distribution in $\mathcal{E}$
(choosing some orthonormal basis in $\mathcal{E}$ the distribution can be found
by taking iid standard normal random variables for the resulting coordinates).
This distribution on $\mathcal{E}$ induces a distribution on the Weyl chamber
    \[ W_n = \{x\in\text{R}^n\mid x_1\leq x_2\leq\dots\leq x_n \} , \]
through the function that maps a matrix $A\in\mathcal{E}$ to its vector of 
ordered eigenvalues $\text{eig}(A)$.
We slightly abuse notation and also denote this induced
distribution on $W_n$ by $\text{G}\beta\text{E}$ (from the context it is clear
whether we are talking about matrices or eigenvalues; we will mostly be talking
about the eigenvalues).
The norm on $\mathcal{E}$ is given by the (Euclidean) norm on the eigenvalues,
$\|A\|=\|\text{eig}(A)\|$.

We denote the cone of positive semidefinite matrices by $C\subset\mathcal{E}$.
The rank decomposition of $\mathcal{E}$ induces a rank decomposition of $C$,
which we denote by
    \[ C = \bigcup_{r=0}^n M_r . \]
While the rank function is important, we will focus in this note on the index
function that counts the number of positive entries in a vector,
    \[ \text{ind}(x)=(\text{number of positive entries in }x) . \]
Instead of the rank we can also use the index function to describe the
strata of $C$,
    \[ M_r = \{A\in C\mid \text{ind}(\text{eig}(A))=r \} . \]

## Curvature measures of symmetric cones {#curv_meas}

The intrinsic volumes of a convex cone, and also the curvature measures that
we will analyze here, can be described through corresponding
(bivariate) chi-bar-squared distributions. For this we denote the orthogonal
projection on the cone $C$ by $\Pi_C\colon\mathcal{E}\to C$,
    \[ \Pi_C(A) = \text{argmin}\{ \|A-B\| \mid B\in C \} . \]
The eigenvalues of the projection on $C$ and on its polar cone $C^\circ=-C$
are given by
    \[ \text{eig}\big(\Pi_C(A)\big) = \big(\text{eig}(A)\big)_+ ,\quad 
        \text{eig}\big(\Pi_{C^\circ}(A)\big) = \big(\text{eig}(A)\big)_- , \]
where in $(x)_+=:x_+$ all negative entries of $x$ are replaced by zero, and similarly
in $(x)_-=:x_-$ all positive entries of $x$ are replaced by zero. To simplify the 
notation we will now focus on the eigenvalues of the matrices.

The bivariate chi-bar-squared distribution of $C$ is the distribution of the pair
    \[ \big(\|x_+\|^2,\|x_-\|^2\big) ,\quad x\sim \text{G}\beta\text{E} . \]
The connection to the intrinsic volumes $v_k:=v_k(C)$, $0\leq k\leq d$, is given by an
alternative way to describe this distribution: Let the discrete random variable
$Z\in\{0,\ldots,d\}$ be described by the probabilities
    \[ \text{Prob}(Z=k) = v_k , \]
then $(X_+,X_-)$ defined by the conditional distributions
    \[ X_+\mid Z\sim \chi_Z^2,\quad X_-\mid Z\sim \chi_{d-Z}^2 ,\quad
        X_+\mid Z,\; X_-\mid Z \text{ independent} , \]
has the same distribution as $\big(\|x_+\|^2,\|x_-\|^2\big)$.
This property, known as the generalized Steiner formula, provides a characterization
of the intrinsic volumes.

The curvature measures $\Phi_{kr} := \Phi_k(C,M_r)$ allow for a similar characterization
if the $\text{G}\beta\text{E}$ is conditioned on the index. More precisely,
if $\emptyset\neq R\subseteq\{0,\ldots,n\}$, then the pair
    \[ \big(\|x_+\|^2,\|x_-\|^2\big) ,\quad
        x\sim \text{G}\beta\text{E}\mid\text{ind}(x)\in R \]
has the same distribution as $(X_+,X_-)$,
    \[ X_+\mid Z\sim \chi_Z^2,\quad X_-\mid Z\sim \chi_{d-Z}^2 ,\quad
        X_+\mid Z,\; X_-\mid Z \text{ independent} , \]
if the latent variable $Z$ has the probabilities
    \[ \text{Prob}(Z=k) =
    \frac{\sum_{r\in R} \Phi_{kr}}{\sum_{\ell=0}^d \sum_{r\in R} \Phi_{\ell r}} . \]
Again, this property, a consequence of a localized form of the general Steiner formula,
characterizes the curvature measures $\Phi_{kr}$.
We will discuss the issue of the normalizations $\sum_{\ell=0}^d \sum_{r\in R} \Phi_{\ell r}$
below, but before doing this we explain some important inequalities giving the range
of the indices $k,r$ such that $\Phi_{kr}$ is nonzero.

### Pataki bounds

The curvature measure $\Phi_{kr}$ is nonzero iff the indices $k,r$ satisfy the inequalities
\begin{align*}
    r + \beta \binom{r}{2} & \leq k \leq r + \beta \binom{r}{2} + \beta r (n-r)
    = d - \Big( n-r + \beta \binom{n-r}{2} \Big)
\\ \iff & 
    \begin{cases}
        \frac{r^2}{2}(1+\frac{1}{r}) \leq k \leq d
        - \frac{(n-r)^2}{2} (1+\frac{1}{n-r}) & \text{if } \beta=1
    \\ r^2 \leq k \leq d-(n-r)^2
        & \text{if } \beta=2
    \\ 2r^2 (1-\frac{1}{2r}) \leq k \leq d
        - 2(n-r)^2 (1-\frac{1}{2(n-r)}) & \text{if } \beta=4
    \end{cases}
\end{align*}
Rewriting these inequalities in terms of $\frac{k}{d}$ yields
\begin{align*}
    & \frac{r + \beta \binom{r}{2}}{d} \leq \frac{k}{d} \leq 
        1 - \frac{n-r + \beta \binom{n-r}{2}}{d}
\\ \iff & 
    \begin{cases}
        \frac{1+\frac{1}{r}}{1+\frac{1}{n}}\big(\frac{r}{n}\big)^2 \leq \frac{k}{d} \leq 1
        - \frac{1+\frac{1}{n-r}}{1+\frac{1}{n}} \big(1-\frac{r}{n}\big)^2 & \text{if } \beta=1
    \\ \big(\frac{r}{n}\big)^2 \leq \frac{k}{d} \leq 1-\big(1-\frac{r}{n}\big)^2
        & \text{if } \beta=2
    \\ \frac{1-\frac{1}{2r}}{1-\frac{1}{2n}}\big(\frac{r}{n}\big)^2 \leq \frac{k}{d} \leq 1
        - \frac{1-\frac{1}{2(n-r)}}{1-\frac{1}{2n}}\big(1-\frac{r}{n}\big)^2 & \text{if } \beta=4
    \end{cases}
\end{align*}
Asymptotically, we obtain for $n\to\infty$ and $r_n,k_n$ such that
$\frac{r_n}{n}\to\rho\in(0,1)$ and $\frac{k_n}{d}\to\kappa\in(0,1)$,
the inequalities
    \[ \rho^2 \leq \kappa\leq 1-(1-\rho)^2 ,\quad \text{for } \beta=1,2,4 . \]
We illustrate these inequalities for $n=3,6,10$, $\beta=1,2,4$:
(the asymptotic bounds
$(\frac{r}{n})^2 \leq \frac{k}{d} \leq 1-(1-\frac{r}{n})^2$
are indicated by the dashed curves)

```{r illustr-pat-bds, fig.width = 7, fig.height=12, echo=FALSE}

illustrBds <- function(beta,n) {
    
    n_ray <- sum(P[,k]==1)
    n_full <- sum(P[,k]>1)
    sdim <- Sdim_sm[ P[1,k]-1, ]
    var  <- Var_sm[  P[1,k]-1, ]
    for (l in 2:n_full) {
        sdim <- c(rep(1,N_sm+1) %*% t(sdim) + Sdim_sm[ P[l,k]-1, ])
        var  <- c(rep(1,N_sm+1) %*% t(var)  + Var_sm[  P[l,k]-1, ])
    }
    sdim <- sdim + n_ray/2
    var  <- var  + n_ray/4
    return(geom_point(data=tibble(sdim,var), aes(sdim,var), size=1, alpha=0.05))
}

plotsPB <- list()
i <- 0
for (beta in c(1,2,4)) {
    for (n in c(3,6,10)) {
        i <- i+1
        d <- n + beta*choose(n,2)
        n_grid <- 0:n
        d_grid <- 0:d
        pts <- expand.grid(d_grid,n_grid)
        colnames(pts) <- c("k","r")
        pts$in_supp <- 
            pts$k>=(pts$r + beta*choose(pts$r,2)) & 
            pts$k<=(pts$r + beta*(choose(pts$r,2)+pts$r*(n-pts$r)))
        y_pat <- seq(0,1,length.out=1e2)
        pat_bd_low <- tibble(patX=d*y_pat^2, patY=n*y_pat)
        pat_bd_upp <- tibble(patX=d*(1-y_pat^2), patY=n*(1-y_pat))
        plotsPB[[i]] <- ggplot() + 
            geom_point(data=pts, aes(x=r, y=k, color=in_supp), alpha=0.7) +
            theme_bw() +
            xlab(str_c("n=",n,", beta=",beta," (d=",n+beta*choose(n,2),")")) +
            theme(legend.position="none", 
                # axis.title.x=element_blank(),
                axis.title.y=element_blank(),
                axis.text.x=element_blank(), axis.text.y=element_blank(),
                axis.ticks.x=element_blank(), axis.ticks.y=element_blank(),
                panel.border = element_blank(), panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
                axis.line = element_line(colour = "black")
                ) +
            scale_color_manual(values=c("gray60", "black")) +
            geom_line(data=pat_bd_low, aes(patY, patX), linetype=2, alpha=0.5) +
            geom_line(data=pat_bd_upp, aes(patY, patX), linetype=2, alpha=0.5)
    }
}
multiplot(plotlist = plotsPB, cols = 3)
```

### Curvature measures normalized through index constraints

As explained above, by restricting the index to be equal to $r$, that is,
$R=\{r\}$, we obtain the connection between the conditioned eigenvalue
distribution $\text{G}\beta\text{E}\mid \text{ind}=r$ and the index normalized
curvature measures
    \[ \Phi^{\text{ind}}_{kr} := \frac{\Phi_{kr}}{\sum_{\ell=0}^d \Phi_{\ell r}} ,\quad
  k=0,\ldots,d . \]
Assuming that we have reconstructed these index normalized curvature measures
as well as the intrinsic volumes $v_k$, we can reconstruct the unnormalized
curvature measures: summing the curvature measures over $r$ yields the intrinsic volumes,
    \[ v_k = \sum_{r=0}^n \Phi_{kr} = \sum_{r=0}^n\Phi^{\text{ind}}_{kr} \sum_{\ell=0}^d \Phi_{\ell r} ; \]
solving this overdetermined linear system of equations gives
    \[ \bigg(\sum_{j=0}^d \Phi_{jr}\bigg) = 
            \big(\Phi^{\text{ind}}_{kr}\big)^\dagger\cdot\big(v_k\big) , \]
where $.^\dagger$ denotes Moore-Penrose inversion, and thus
    \[ \big(\Phi_{kr}\big) = \big(\Phi^{\text{ind}}_{kr}\big)\cdot
            \text{diag}\Big(\big(\Phi^{\text{ind}}_{kr}\big)^\dagger\cdot\big(v_k\big)\Big) . \]
The problem with this approach is that it is not pracical for even moderate
sizes due to the double concentration in both $r$ and $k$ around $n/2$ and $d/2$,
respectively, resulting in serious numerical issues.

But the unnormalized curvature measures are arguably not too interesting anyway,
due to these concentration effects. Much more interesting are the curvature
measures, which are normalized with respect to the other index; see
[@AB15s] for applications in semidefinite programming. We call these
*dimension normalized* curvature measures,
  \[ \Phi^{\text{dim}}_{kr} := \frac{\Phi_{kr}}{\sum_{s=0}^n \Phi_{ks}} ,\quad
  r=0,\ldots,n,\quad k=0,\ldots,d . \]
One might think of finding these from the index normalized curvature measures
through the above described way of reconstructing the unnormalized curvature
measures. But since this procedure is not numerically stable, we propose a 
different procedure as follows.

In addition to the index normalized curvature
measures, which correspond to the choice $|\mathcal{R}|=1$, we assume to have
reconstructed those corresponding to the choice $|\mathcal{R}|=2$.
For notational convenience, fix $k\in\{0,\ldots,d\}$ and let us denote
\begin{align*}
    a_r & := \Phi^{\text{ind}}_{kr} = \frac{\Phi_{kr}}{\sum_{j=0}^d \Phi_{jr}} ,
    & r & =0,\ldots,n
\\ b_r & := \frac{\Phi_{kr}+\Phi_{k,r+1}}{\sum_{j=0}^d \big(\Phi_{jr}
    +\Phi_{j,r+1}\big)} , & r & =0,\ldots,n-1
\\ c_r & := \frac{\Phi_{k,r+1}}{\Phi_{kr}} , & r & =0,\ldots,n-1 .
\end{align*}
Then we obtain
\begin{align*}
    \frac{1}{b_r} & = 
    \frac{\sum_{j=0}^d \Phi_{jr}}{\Phi_{kr}}\frac{\Phi_{kr}}{\Phi_{kr}+\Phi_{k,r+1}}
    + \frac{\sum_{j=0}^d \Phi_{j,r+1}}{\Phi_{k,r+1}} \frac{\Phi_{k,r+1}}{\Phi_{kr}+\Phi_{k,r+1}}
\\ & = \frac{1}{a_r}\frac{1}{1+c_r} + \frac{1}{a_{r+1}}\frac{1}{\frac{1}{c_r}+1} .
\end{align*}
Solving this for $c_r$ yields
  \[ c_r = -\frac{\frac{1}{b_r}-\frac{1}{a_r}}{\frac{1}{b_r}-\frac{1}{a_{r+1}}}
  = -\frac{a_{r+1}\big(a_r-b_r\big)}{a_r\big(a_{r+1}-b_r\big)} .
  \]
Using this formula to reconstruct $c_r$ it is now clear how to reconstruct
the dimension normalized intrinsic volumes: for given dimension $k$,

1. find $r_0=\text{argmax}\{ \Phi^{\text{ind}}_{kr}\mid 0\leq r \leq n \}$  
    (this step is try to minimize numerical issues; in theory any index within
    the Pataki bounds would work)
2. set $\phi_{r_0}=1$ and define
\begin{align*}
    \phi_{r_0+1} & := c_{r_0}, & \phi_{r_0+2} & := c_{r_0}c_{r_0+1},
    & \phi_{r_0+3} & := c_{r_0}c_{r_0+1}c_{r_0+2}, & \dots
\\  \phi_{r_0-1} & := \frac{1}{c_{r_0-1}},
    & \phi_{r_0-2} & := \frac{1}{c_{r_0-1}c_{r_0-2}},
    & \phi_{r_0-3} & := \frac{1}{c_{r_0-1}c_{r_0-2}c_{r_0-3}}, & \dots
\end{align*}
    (stay within the Pataki bounds, of course)
3. the dimension normalized curvature measures are then given by
    \[ \Phi^{\text{dim}}_{kr} = \frac{\phi_r}{\sum_{s=0}^n \phi_s} ,\quad r=0,\ldots,n . \]


## Implementations

Having explained the general approach, it remains to implement the computations,
which consist of two parts:

1. sampling from the index constrained eigenvalue distribution,
2. reconstructing the index normalized curvature measures from sampling data.

The difficulty of the first step lies in the again in the concentration effects.
A random vector from the $\text{G}\beta\text{E}$ will have its index close to
$n/2$ with high probability, making a simple rejection sampler
(sample from $\text{G}\beta\text{E}$ then reject if the index is not as needed)
a practically infeasible approach. Instead we will use the Hamiltonian Monte-Carlo
sampler [Stan](http://mc-stan.org/)
([wikipedia](https://en.wikipedia.org/wiki/Stan_(software)))
for this task.

The second step is solved by adapting the expectation maximization (EM) approach
for reconstructing the intrinsic volumes to the situation at hand.
The required changes are minimal and and described below.


### Sampling index constrained eigenvalues

The sampler [Stan](http://mc-stan.org/) works solely through the log-likelihood
function, which only has to be given up to additive constant (so the normalizing constant
for the density does not have to be specified). Concretely, the density of
$x\sim\text{G}\beta\text{E}$ is given by
    \[ p(x) \propto e^{-\|x\|^2/2} \prod_{i<j} \big|x_i-x_j\big|^\beta . \]
The corresponding log-likelihood is easily computed in Stan; the restrictions on
the index is realized by grouping the ordered eigenvalues into "positive", "free",
"negative" and by requiring that the positive and negative ones do not change sign
and the free eigenvalues lie between the two groups. Concretely, if $n=40$ and
the index shall lie between $25$ and $35$, then we assume that the eigenvalues
$x_1\leq x_2 \leq \dots \leq x_{35}$ are grouped into $5$ eigenvalues with negative
sign, $x_1,\ldots,x_5$, $10$ eigenvalues with no prescribed sign,
$x_6,\ldots,x_{15}$, and the remaining $25$ eigenvalues with positive sign,
$x_{16},\ldots,x_{40}$.

The function `constr_eigval` from the `symconivol` package generates the Stan model
for the index constrained eigenvalue distribution. The models are slightly different
for whether there are "positive", "free", or "negative",
eigenvalues allowed. Passing this to the Stan sampler and then running it
with the data consisting of the Dyson index $\beta$ and then the numbers of
positive/free/negative eigenvalues, yields samples from the resulting index
constrained eigenvalue distribution.

**Example computations:**

The following lines will construct a model for negative, 
free, and positive eigenvalues, then run it for $5$ negative, $10$ free,
and $25$ positive eigenvalues, and then extract the sampled eigenvalues:
```{r constr_evals, eval=FALSE}
filename <- "tmp.stan"
constr_eigval( pos=TRUE, free=TRUE, neg=TRUE, filename=filename, overwrite=TRUE )
data = list( beta=1, nn=5, nf=10, np=25 )
stan_samp <- stan( file = filename, data = data, chains = 1, warmup = warmup,
                   iter = 1e5, cores = 2, refresh = 1e4 )
file.remove(filename)
samp <- list( ep = rstan::extract(stan_samp)$ep,
              ef = rstan::extract(stan_samp)$ef,
              en = rstan::extract(stan_samp)$en )
```
The resulting empirical eigenvalue distribution(s) look as follows:
```{r disp-eigv-01, echo=FALSE}
img_dpi <- 300
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=5_10_25.png", dpi=img_dpi)
```

To illustrate the different situations we illustrate these empirical eigenvalue
distributions for some more values for the parameters:

* **$0$ negative, $15$ free, $25$ positive eigenvalues:**
```{r disp-eigv-02, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=0_15_25.png", dpi=img_dpi)
```

* **$15$ negative, $0$ free, $25$ positive eigenvalues:**
```{r disp-eigv-03, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=15_0_25.png", dpi=img_dpi)
```

* **$35$ negative, $5$ free, $0$ positive eigenvalues:**
```{r disp-eigv-04, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=35_5_0.png", dpi=img_dpi)
```

* **$20$ negative,  $0$ free, $0$ positive eigenvalues:**
```{r disp-eigv-05, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=20_0_0.png", dpi=img_dpi)
```

* **$0$ negative, $40$ free, $0$ positive eigenvalues:**
```{r disp-eigv-06, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=0_40_0.png", dpi=img_dpi)
```

* **$0$ negative, $0$ free, $40$ positive eigenvalues:**
```{r disp-eigv-07, echo=FALSE}
include_graphics("curv-meas-from-constr-evals_figures/nn_nf_np=0_0_40.png", dpi=img_dpi)
```

## Reconstructing curvature measures

The weights of a bivariate chi-bar-squared distribution
can be reconstructed as described in the vignette
[Estimating conic intrinsic volumes from bivariate chi-bar-squared data](../../conivol/vignettes/estim-conic-intrinsic-volumes-with-EM.html)
from the `conivol` package.
We explain again the main idea behind this algorithm, as well as the differences
between reconstructing these curvature measures and reconstructing intrinsic
volumes.

### Bivariate chi-bar-squared data, latent variable, likelihood, EM algorithm

In the following we will assume that the index constraints are of the form
$r_0\leq \text{ind}(x)\leq r_1$, that is, $R=\{r_0,\ldots,r_1\}$, and we
assume $r_0>0$ and $r_1<d$.
As explained [above](#curv_meas), we can turn a sample from the index constrained
Gaussian orthogonal/unitary/symplectic ensemble by taking the squared norms of
the positive and negative components,
    \[ (X,Y) = \big( \|x_+\|^2, \|x_-\|^2 \big) . \]
The distribution of $(X,Y)$ can then be described in terms of the latent variable
$Z$,
    \[ \text{Prob}(Z=k) =
    \frac{\sum_{r=r_0}^{r_1} \Phi_{kr}}{\sum_{\ell=0}^d \sum_{r=r_0}^{r_1}
    \Phi_{\ell r}} , \]
through the conditional distributions
    \[ X\mid Z\sim \chi_Z^2,\quad Y\mid Z\sim \chi_{d-Z}^2 ,\quad
        X\mid Z,\; Y\mid Z \text{ independent} . \]

The Pataki bounds show that $\sum_{r=r_0}^{r_1} \Phi_{kr}$ is nonzero iff
  \[ r_0+\beta\binom{r_0}{2} \leq k \leq d - \Big( n-r_1 + \beta \binom{n-r_1}{2} \Big) . \]
In particular, since we assume that the index is not $0$ or $d$,
the latent variable $Z$ will not
take the values $0$ or $d$ with positive probability. So the latent variable is
indeed entirely hidden, which is different from the intrinsic volumes case and
simplifies the situation here.

Assuming that we have $N$ samples
$(\mathbf{X},\mathbf{Y})=\big((X_1,Y_1),\ldots,(X_N,Y_N)\big)$
the likelihood function, up to normalizing constant, is given by
    \[ L(\Phi\mid \mathbf{X},\mathbf{Y}) \propto
    \prod_{i=1}^N \sum_{k=1}^d f_{ik}\sum_{r=r_0}^{r_1}\Phi_{kr} ,\qquad
    f_{ik} = f_k(X_i) f_{d-k}(Y_i) , \]
where $f_k(x)$ denotes the density of the chi-squared distribution,
$f_k(x)\propto x^{k/2-1}e^{-x/2}$. Taking the latent variable
into account, we obtain
    \[ L(\Phi\mid \mathbf{X},\mathbf{Y},\mathbf{Z}) \propto
    \prod_{i=1}^N \prod_{k=1}^d
    \bigg( f_{ik}\sum_{r=r_0}^{r_1}\Phi_{kr} \bigg)^{(Z_i=k)} . \]

The EM algorithm tries to find the maximum likelihood estimate of the parameters
of the latent variable by maximizing the conditional likelihood function, with the
likelihood of the data expressed in the latent variable, which are conditioned
on the current iterate. Concretely, we express the posterior density for the $i$th
latent variable in the form
    \[ p\big(Z_i=k\mid X_i,Y_i\big) \propto
        p\big(Z_i=k\big) \, p\big(X_i,Y_i\mid Z_i=k\big)
        = f_{ik}\sum_{r=r_0}^{r_1}\Phi_{kr} . \]
The EM algorithm then finds the $(t+1)$th iterate $\Phi^{(t+1)}$ from the
$t$th iterate $\Phi^{(t)}$ by maximizing the following function in $\Phi$:
    \[ \underset{\mathbf{Z}\mid \mathbf{X},\mathbf{Y},\Phi^{(t)}}{\text{E}}
    \big[\log L\big(\Phi \,\big|\, \mathbf{X},\mathbf{Y},
    \mathbf{Z} \big)\big] = \sum_{k=1}^{d-1}
    \Big(\sum_{r=r_0}^{r_1}\Phi_{kr}^{(t)}\Big)
    \Big(\sum_{i=1}^N f_{ik}\Big)
    \log\Big(\sum_{r=r_0}^{r_1}\Phi_{kr}\Big)
    + \text{const} . \]
This function, which we divide by the number of samples to achieve a convergence
of the constants,
  \[ F(a_1,\ldots,a_{d-1}) = \sum_{k=1}^{d-1}
    \Big(\sum_{r=r_0}^{r_1}\Phi_{kr}^{(t)}\Big)
    \Big(\frac{1}{N}\sum_{i=1}^N f_{ik}\Big)
    \log(a_k) , \]
can be maximized over the probability simplex as a separable convex program.
In the `symconivol` package this is achieved via [MOSEK](https://www.mosek.com/).

**Example computations:**



### Log-concavity

Note that a useful assumption is here that the weights form a log-concave sequence.
Although this is not known (yet) for these curvature measures, we will use this
assumption in the concrete reconstructions later on.


## Putting everything together

do stuff



## References




