<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Study on curvature measures of symmetric cones • symconivol</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Study on curvature measures of symmetric cones">
<meta property="og:description" content="">
<meta property="og:image" content="http://damelunx.github.io/symconivol/logo.png">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">symconivol</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="../reference/symconivol.html">Overview</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Study
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/curv_meas.html">Main</a>
    </li>
    <li>
      <a href="../articles/curv_meas_tech.html">Technical addendum</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="../reference/index.html">
    <span class="fa fa-file-code-o"></span>
     
    functions &amp; data
  </a>
</li>
<li>
  <a href="https://github.com/damelunx/symconivol">
    <span class="fa fa-github fa-lg"></span>
     
    github
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Study on curvature measures of symmetric cones</h1>
                        <h4 class="author">Dennis Amelunxen</h4>
            
            <h4 class="date">27 March, 2018</h4>
          </div>

    
    
<div class="contents">
<p>In this note we study the curvature measures of symmetric cones through the distribution of the Gaussian orthogonal/unitary/symplectic ensemble conditioned on the index function, that is, on the number of positive eigenvalues.</p>
<p>Our approach follows closely the approach of reconstructing the conic intrinsic volumes from the corresponding bivariate chi-bar-squared distribution. We will assume familiarity with this approach, which is explained in the vignette <a href="https://damelunx.github.io/conivol/articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a> from the <code>conivol</code> package.</p>
<p>We present the connection between the curvature measures and the index constrained Gaussian orthogonal/unitary/symplectic ensemble without proof; for more information and references, see <span class="citation">(Amelunxen and Bürgisser <a href="#ref-AB15s">2015</a>)</span>. One motivation for this study is its connection with the problem of predicting the rank of the solution of a random semidefinite program. This connection is explained in the <a href="#appl_SDP">final section</a> of this note.</p>
<p>In an <a href="#alg_deg">appendix</a> we present an observation about a possible connection between the curvature measures of semidefinite cones and the algebraic degree of semidefinite programming.</p>
<p>Some plots in this study require longer computations and are included as image files; the code for getting the data and for constructing these images is provided in <a href="curv_meas_tech.html">this technical note</a>.</p>
<p><strong>Vignettes from the <code>conivol</code> package:</strong></p>
<ul>
<li>
<a href="https://damelunx.github.io/conivol/articles/conic-intrinsic-volumes.html">Conic intrinsic volumes and (bivariate) chi-bar-squared distribution</a>: introduces conic intrinsic volumes and (bivariate) chi-bar-squared distributions, as well as the computations involving polyhedral cones,</li>
<li>
<a href="https://damelunx.github.io/conivol/articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a>: describes the details of the algorithm for finding the intrinsic volumes of closed convex cones from samples of the associated bivariate chi-bar-squared distribution,</li>
<li>
<a href="https://damelunx.github.io/conivol/articles/bayesian.html">Bayesian estimates for conic intrinsic volumes</a>: describes the Bayesian approach for reconstructing intrinsic volumes from sampling data, which can either be samples from the intrinsic volumes distribution (in the case of polyhedral cones), or from the bivariate chi-bar-squared distribution, and which can be with or without enforcing log-concavity of the intrinsic volumes.</li>
</ul>
<div id="symm_cones" class="section level2">
<h2 class="hasAnchor">
<a href="#symm_cones" class="anchor"></a>Symmetric cones</h2>
<p><a href="https://en.wikipedia.org/wiki/Symmetric_cone">Symmetric cones</a> are self-dual convex cones with a transitive group of symmetries. Every symmetric cone decomposes into an orthogonal sum of a finite number of simple symmetric cones, which consist of</p>
<ul>
<li>the Lorentz cones (circular cones of radius <span class="math inline">\(\pi/4\)</span>),</li>
<li>positive semdefinite real symmetric matrices,</li>
<li>positive semdefinite complex unitary matrices,</li>
<li>positive semdefinite quaternion unitary matrices,</li>
<li>positive semidefinite octonion unitary <span class="math inline">\(3\times 3\)</span>-matrices.</li>
</ul>
<p>The first four points each describe families of cones, while the last one is just a single exceptional cone. The Lorentz cones have a very simple structure (arguably the simplest kind of convex cones, which are not linear subspaces) and do not have the kind of structure that we focus on in this note. We also do not discuss the exceptional symmetric cone. So “symmetric cone” in this note is synonymous to a cone of positive semidefinite real symmetric/complex unitary/quaternion unitary matrices of a certain format.</p>
<p>We use the Dyson index <span class="math inline">\(\beta\in\{1,2,4\}\)</span> to indicate whether the ground field is real (<span class="math inline">\(\beta=1\)</span>), complex (<span class="math inline">\(\beta=2\)</span>), or quaternion (<span class="math inline">\(\beta=4\)</span>); we denote the size of the matrices by <span class="math inline">\(n\)</span>. The set of real symmetric/complex unitary/quaternion unitary matrices form a Euclidean space <span class="math inline">\(\mathcal{E}\)</span> of dimension <span class="math display">\[ d = n + \beta\binom{n}{2}
        = \frac{\beta}{2} n^2 \cdot \begin{cases} (1+\frac{1}{n}) &amp; \text{if } \beta=1
            \\ 1 &amp; \text{if } \beta=2
            \\ (1-\frac{1}{2n}) &amp; \text{if } \beta=4 .
            \end{cases} \]</span> The Gaussian orthogonal/symmetric/unitary ensemble, which we denote by <span class="math inline">\(\text{G}\beta\text{E}\)</span>, is the standard normal distribution on <span class="math inline">\(\mathcal{E}\)</span> (after choosing some orthonormal basis in <span class="math inline">\(\mathcal{E}\)</span>, the distribution can be found by taking iid standard normal random variables for the resulting coordinates). This distribution on <span class="math inline">\(\mathcal{E}\)</span> induces a distribution on the Weyl chamber <span class="math display">\[ \{x\in\text{R}^n\mid x_1\leq x_2\leq\dots\leq x_n \} , \]</span> through the function that maps a matrix <span class="math inline">\(A\in\mathcal{E}\)</span> to its vector of ordered eigenvalues <span class="math inline">\(\text{eig}(A)\)</span>. We slightly abuse notation and also denote this induced distribution on the Weyl chamber by <span class="math inline">\(\text{G}\beta\text{E}\)</span> (from the context it is clear whether we are talking about matrices or eigenvalues; we will mostly be talking about the eigenvalues). The Euclidean norm on <span class="math inline">\(\mathcal{E}\)</span> can be expressed through the eigenvalues; in fact, it is given by the Euclidean norm of the eigenvalues, <span class="math inline">\(\|A\|_F=\|\text{eig}(A)\|=\|\text{eig}(A)\|_2\)</span>. (We use the notation <span class="math inline">\(\|A\|_F\)</span> to emphasize that we are not taking the usual operator norm but the Frobenius norm, which is the Euclidean norm in matrix space.)</p>
<p>We denote the cone of positive semidefinite matrices by <span class="math inline">\(\mathcal{C}\subset\mathcal{E}\)</span>. The rank decomposition of <span class="math inline">\(\mathcal{E}\)</span> induces a rank decomposition of <span class="math inline">\(\mathcal{C}\)</span>, which we denote by <span class="math display">\[ \mathcal{C} = \bigcup_{r=0}^n M_r . \]</span> While the rank function is important, we will focus in this note on the index function that counts the number of positive entries in a vector, <span class="math display">\[ \text{ind}(x)=(\text{number of positive entries in }x) . \]</span> Instead of the rank we can also use the index function to describe the strata of <span class="math inline">\(\mathcal{C}\)</span>, <span class="math display">\[ M_r = \{A\in \mathcal{C}\mid \text{ind}(\text{eig}(A))=r \} . \]</span></p>
</div>
<div id="curv_meas" class="section level2">
<h2 class="hasAnchor">
<a href="#curv_meas" class="anchor"></a>Curvature measures of symmetric cones</h2>
<p>The intrinsic volumes of a convex cone, and also the curvature measures that we will analyze here, can be described through corresponding (bivariate) chi-bar-squared distributions. For this we denote the orthogonal projection on the cone <span class="math inline">\(\mathcal{C}\)</span> by <span class="math inline">\(\Pi_{\mathcal{C}}\colon\mathcal{E}\to \mathcal{C}\)</span>, <span class="math display">\[ \Pi_{\mathcal{C}}(A) = \text{argmin}\{ \|A-B\|_F \mid B\in \mathcal{C} \} . \]</span> It is easily verified that the eigenvalues of the projection on the positive semidefinite cone <span class="math inline">\(\mathcal{C}\)</span> and on its polar cone <span class="math inline">\(\mathcal{C}^\circ=-\mathcal{C}\)</span> are given by <span class="math display">\[ \text{eig}\big(\Pi_{\mathcal{C}}(A)\big) = \big(\text{eig}(A)\big)_+ ,\quad
        \text{eig}\big(\Pi_{\mathcal{C}^\circ}(A)\big) = \big(\text{eig}(A)\big)_- , \]</span> where in <span class="math inline">\((x)_+=:x_+\)</span> all negative entries of <span class="math inline">\(x\)</span> are replaced by zero, and similarly in <span class="math inline">\((x)_-=:x_-\)</span> all positive entries of <span class="math inline">\(x\)</span> are replaced by zero. To simplify the notation we will now focus on the eigenvalues of the matrices.</p>
<p>The bivariate chi-bar-squared distribution of <span class="math inline">\(\mathcal{C}\)</span> is the distribution of the pair <span class="math display">\[ \big(\|x_+\|^2,\|x_-\|^2\big) ,\quad x\sim \text{G}\beta\text{E} . \]</span> The connection to the intrinsic volumes <span class="math inline">\(v_k:=v_k(\mathcal{C})\)</span>, <span class="math inline">\(0\leq k\leq d\)</span>, is given by an alternative way to describe this distribution: Let the discrete random variable <span class="math inline">\(Z\in\{0,\ldots,d\}\)</span> be described by the probabilities <span class="math display">\[ \text{Prob}(Z=k) = v_k , \]</span> then <span class="math inline">\((X_+,X_-)\)</span> defined by the conditional distributions <span class="math display">\[ X_+\mid Z\sim \chi_Z^2,\quad X_-\mid Z\sim \chi_{d-Z}^2 ,\quad
        X_+\mid Z,\; X_-\mid Z \text{ independent} , \]</span> has the same distribution as <span class="math inline">\(\big(\|x_+\|^2,\|x_-\|^2\big)\)</span>. This property, known as the generalized Steiner formula, provides a characterization of the intrinsic volumes.</p>
<p>The curvature measures <span class="math inline">\(\Phi_{kr} := \Phi_k(\mathcal{C},M_r)\)</span> allow for a similar characterization if the <span class="math inline">\(\text{G}\beta\text{E}\)</span> is conditioned on the index. More precisely, if <span class="math inline">\(\emptyset\neq R\subseteq\{0,\ldots,n\}\)</span>, then the pair <span class="math display">\[ \big(\|x_+\|^2,\|x_-\|^2\big) ,\quad
        x\sim \big( \text{G}\beta\text{E}\mid\text{ind}(x)\in R \big) \]</span> has the same distribution as <span class="math inline">\((X_+,X_-)\)</span>, hierarchically defined via <span class="math display">\[ X_+\mid Z\sim \chi_Z^2,\quad X_-\mid Z\sim \chi_{d-Z}^2 ,\quad
        X_+\mid Z,\; X_-\mid Z \text{ independent} , \]</span> where the latent variable <span class="math inline">\(Z\)</span> has the probabilities <span class="math display">\[ \text{Prob}(Z=k) =
    \frac{\sum_{r\in R} \Phi_{kr}}{\sum_{\ell=0}^d \sum_{r\in R} \Phi_{\ell r}} . \]</span> Again, this property, a consequence of a localized form of the generalized Steiner formula, characterizes the curvature measures <span class="math inline">\(\Phi_{kr}\)</span>.</p>
<p>Note that for <span class="math inline">\(R=\{0,\ldots,n\}\)</span> we obtain the intrinsic volumes, <span class="math display">\[ \sum_{r=0}^n \Phi_{kr}=v_k . \]</span> On the other hand, summing over the “dimension” parameter <span class="math inline">\(k\)</span> yields the distribution of the index function, <span class="math display">\[ \sum_{k=0}^d \Phi_{kr} = \text{Prob}\{\text{ind}(x)=r\} ,\quad
    x\sim \text{G}\beta\text{E} . \]</span> Yet another characterization of the curvature measures is obtained through the rank distribution of the solution of a (Gaussian) random semidefinite program; see the <a href="#appl_SDP">final section</a> of this note for more details.</p>
<div id="pat_bnds" class="section level3">
<h3 class="hasAnchor">
<a href="#pat_bnds" class="anchor"></a>Pataki bounds</h3>
Not all curvature measures <span class="math inline">\(\Phi_{kr}\)</span>, <span class="math inline">\(0\leq k\leq d\)</span>, <span class="math inline">\(0\leq r\leq n\)</span>, are nonzero. In fact, the support of <span class="math inline">\(\Phi_{kr}\)</span> is accurately described by two inequalities known as Pataki’s Inequalities: The curvature measure <span class="math inline">\(\Phi_{kr}\)</span> is nonzero iff the indices <span class="math inline">\(k,r\)</span> satisfy the inequalities <span class="math display">\[ r + \beta \binom{r}{2} \leq k
    \quad \text{and}\quad k \leq r + \beta \binom{r}{2} + \beta r (n-r)
    = d - \Big( n-r + \beta \binom{n-r}{2} \Big) . \]</span> Concretely, we obtain in the three cases <span class="math inline">\(\beta=1,2,4\)</span>,
<span class="math display">\[\begin{align*}
    \beta=1: &amp; &amp; \tfrac{r^2}{2}(1+\tfrac{1}{r}) &amp; \leq k \leq d
        - \tfrac{(n-r)^2}{2} (1+\tfrac{1}{n-r}) ,
    \\ \beta=2: &amp; &amp; r^2 &amp; \leq k \leq d-(n-r)^2 ,
    \\ \beta=4: &amp; &amp; 2r^2 (1-\tfrac{1}{2r}) &amp; \leq k \leq d
        - 2(n-r)^2 (1-\tfrac{1}{2(n-r)}) ,
\end{align*}\]</span>
Rewriting these inequalities in terms of <span class="math inline">\(\frac{k}{d}\)</span> yields
<span class="math display">\[\begin{align*}
    &amp; \frac{r + \beta \binom{r}{2}}{d} \leq \frac{k}{d} \leq
        1 - \frac{n-r + \beta \binom{n-r}{2}}{d}
\\ \iff &amp; \left\{
    \begin{array}{r@{\!}c@{\!}ll}
        \frac{1+\frac{1}{r}}{1+\frac{1}{n}}\cdot\big(\frac{r}{n}\big)^2
        &amp; \leq \frac{k}{d} \leq &amp;
        1 - \big(1-\frac{r}{n}\big)^2\cdot \frac{1+\frac{1}{n-r}}{1+\frac{1}{n}} &amp; \text{if } \beta=1
    \\ \big(\frac{r}{n}\big)^2
        &amp; \leq \frac{k}{d} \leq &amp;
        1-\big(1-\frac{r}{n}\big)^2
        &amp; \text{if } \beta=2
    \\ \frac{1-\frac{1}{2r}}{1-\frac{1}{2n}}\cdot\big(\frac{r}{n}\big)^2
        &amp; \leq \frac{k}{d} \leq &amp;
        1 - \big(1-\frac{r}{n}\big)^2\cdot\frac{1-\frac{1}{2(n-r)}}{1-\frac{1}{2n}} &amp; \text{if } \beta=4
    \end{array}\right.
\end{align*}\]</span>
<p>Asymptotically, we obtain for <span class="math inline">\(n\to\infty\)</span> and <span class="math inline">\(r_n,k_n\)</span> such that <span class="math inline">\(\frac{r_n}{n}\to\rho\in[0,1]\)</span> and <span class="math inline">\(\frac{k_n}{d}\to\kappa\in[0,1]\)</span>, the inequalities <span class="math display">\[ \rho^2 \leq \kappa\leq 1-(1-\rho)^2 ,\quad \text{for } \beta=1,2,4 . \]</span> We illustrate these inequalities for <span class="math inline">\(n=3,6,10\)</span>, <span class="math inline">\(\beta=1,2,4\)</span>: (the asymptotic bounds <span class="math inline">\((\frac{r}{n})^2 \leq \frac{k}{d} \leq 1-(1-\frac{r}{n})^2\)</span> are indicated by the dashed curves)</p>
<p><img src="curv-meas_figures/illustr-pat-bds-1.png" width="672" style="display: block; margin: auto;"></p>
<p>The goal of this study is to analyze the distribution of the curvature measures within this Pataki range.</p>
It is known that the index concentrates sharply around <span class="math inline">\(n/2\)</span>, and it is also known that the intrinsic volumes concentrate sharply around <span class="math inline">\(d/2\)</span>. So the unnormalized curvature measures concentrate in both parameters, <span class="math inline">\(r\)</span> and <span class="math inline">\(k\)</span>, around <span class="math inline">\(n/2\)</span> and <span class="math inline">\(d/2\)</span>, respectively. We will thus focus on the <strong>conditional distributions</strong>
<span class="math display">\[\begin{align*}
    \Phi^{\text{ind}}_r &amp; := \frac{(\Phi_{0r},\ldots,\Phi_{dr})}{\sum_{\ell=0}^d \Phi_{\ell r}}
    = \frac{(\Phi_{0r},\ldots,\Phi_{dr})}{\text{Prob}\{\text{ind}(x)=r\}} ,
\\ \Phi^{\text{dim}}_k &amp; := \frac{(\Phi_{k0},\ldots,\Phi_{kn})}{\sum_{s=0}^n \Phi_{ks}}
    = \frac{(\Phi_{k0},\ldots,\Phi_{kn})}{v_k},
\end{align*}\]</span>
<p>which we call the “index normalized” and the “dimension normalized” distributions. The dimension normalized distributions are particularly important for the application in semidefinite programming, as explained in the <a href="#appl_SDP">final section</a> of this note.</p>
</div>
<div id="ind_constr_cm" class="section level3">
<h3 class="hasAnchor">
<a href="#ind_constr_cm" class="anchor"></a>Curvature measures normalized through index constraints</h3>
<p>As explained <a href="#curv_meas">above</a>, by restricting the index to be equal to <span class="math inline">\(r\)</span>, that is, <span class="math inline">\(R=\{r\}\)</span>, we obtain the connection between the conditioned eigenvalue distribution <span class="math inline">\(\text{G}\beta\text{E}\mid \text{ind}=r\)</span> and the index normalized curvature measures <span class="math inline">\(\Phi^{\text{ind}}_{kr}\)</span> via <span class="math display">\[ \big(\|x_+\|^2,\|x_-\|^2\big)
    \stackrel{\text{dist}}{=} \sum_{k=0}^d 1_{Z=k}\big(X_k,Y_{d-k}\big) , \]</span> where <span class="math inline">\(x\sim \text{G}\beta\text{E}\mid \text{ind}=r\)</span>, <span class="math inline">\(\text{Prob}(Z=k)=\Phi^{\text{ind}}_{kr}\)</span>, and <span class="math inline">\(X_k,Y_k\sim\chi_k^2\)</span>, where <span class="math inline">\(Z,X_k,Y_k\)</span> independent.</p>
<p>We can reconstruct these numbers as weights from a bivariate chi-bar-squared distribution, as described in the vignette <a href="https://damelunx.github.io/conivol/articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a> from the <code>conivol</code> package, assuming that we have samples from the conditioned eigenvalue distribution <span class="math inline">\(\text{G}\beta\text{E}\mid \text{ind}=r\)</span>. Below we will discuss the details for these <a href="#sampl_constr_eigval">sampling</a> and <a href="#reconstr_ind">reconstruction</a> procedures.</p>
<p>What remains to be addressed is the question how to obtain the other normalization, the dimension normalized curvature measures. This question is subtle, and if one tries to do this step in a “brute force” way, for example by reconstructing the unnormalized curvature measures first and then renormalizing these, then one will inevitably fail due to the mentioned double concentration behavior of the curvature measures.</p>
<p>Note that what we actually only need is the ratio of neighboring curvature measures, <span class="math inline">\(\Phi^{\text{dim}}_{k,r+1}/\Phi^{\text{dim}}_{kr}\)</span>, because of the additional normalization <span class="math inline">\(\sum_r \Phi^{\text{dim}}_{kr}=1\)</span>. Now, we can write this in terms of the corresponding ratios of the index normalized curvature measures as follows: <span class="math display">\[ \frac{\Phi^{\text{dim}}_{k,r+1}}{\Phi^{\text{dim}}_{kr}} =
    \frac{\Phi_{k,r+1}}{\Phi_{kr}} =
    \frac{\Phi^{\text{ind}}_{k,r+1}}{\Phi^{\text{ind}}_{kr}}
    \frac{\text{Prob}\{\text{ind}(x)=r+1\}}{\text{Prob}\{\text{ind}(x)=r\}} . \]</span> All that we need to know is thus the ratio of the probabilities <span class="math inline">\(\text{Prob}\{\text{ind}(x)=r+1\}\)</span> and <span class="math inline">\(\text{Prob}\{\text{ind}(x)=r\}\)</span>. We can find this ratio by sampling from the conditioned eigenvalue distribution <span class="math inline">\(\text{G}\beta\text{E}\mid \text{ind}\in\{r,r+1\}\)</span>. More precisely, if we have <span class="math inline">\(N^{(r)}\)</span> samples from this distribution, of which <span class="math inline">\(N^{(r)}_0\)</span> have index <span class="math inline">\(r\)</span> and <span class="math inline">\(N^{(r)}_1\)</span> have index <span class="math inline">\(r+1\)</span>, then the ratio <span class="math inline">\(N^{(r)}_1/N^{(r)}_0\)</span> is an unbiased estimate for the above probability ratio.</p>
<p>This gives us the following procedure for the renormalization step (we work with the logarithm to avoid lengthy products): for fixed <span class="math inline">\(k\)</span>,</p>
<ol style="list-style-type: decimal">
<li>choose <span class="math inline">\(r_0\)</span> such that <span class="math inline">\((k,r_0)\)</span> is in the Pataki range, and set <span class="math inline">\(\phi_{r_0}=0\)</span>,</li>
<li>define <span class="math inline">\(\phi_{r_0\pm s}\)</span> (<span class="math inline">\(s&gt;0\)</span> such that <span class="math inline">\((k,r_0\pm s)\)</span> is in the Pataki range) by
<span class="math display">\[\begin{align*}
\phi_{r_0+s} &amp; = \log\Phi^{\text{ind}}_{k,r_0+s} - \log \Phi^{\text{ind}}_{kr_0}
                + \sum_{i=0}^{s-1} \big( \log N^{(r_0+i)}_1 - \log N^{(r_0+i)}_0 \big) ,
\\  \phi_{r_0-s} &amp; = \log\Phi^{\text{ind}}_{k,r_0-s} - \log \Phi^{\text{ind}}_{kr_0}
                + \sum_{i=1}^s \big( \log N^{(r_0-i)}_0 - \log N^{(r_0-i)}_1 \big) ,
\end{align*}\]</span>
</li>
<li>obtain <span class="math inline">\(\Phi^{\text{dim}}_k \approx \frac{\exp(\phi)}{\sum_r \exp(\phi_r)}\)</span>.</li>
</ol>
For higher dimensions we can approximate the renormalization step in the following way: In <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span> it was shown that <span class="math display">\[ \text{Prob}\{\text{ind}(x)=r\} \approx \exp(-\beta n^2 f(\tfrac{r}{n})) , \]</span> for some rate function <span class="math inline">\(f\)</span>. Using this, we can approximate the probability ratio as follows:
<span class="math display">\[\begin{align*}
     \log\Big(\frac{\text{Prob}\{\text{ind}(x)=r+1\}}{\text{Prob}\{\text{ind}(x)=r\}}\Big)
   &amp; \approx -\beta n^2 \big( f(\tfrac{r+1}{n})-f(\tfrac{r}{n})\big)
\\ &amp; \approx -\beta n\, f'(\tfrac{r}{n}) .
\end{align*}\]</span>
<p>Moreover, we can use this to approximate the values of <span class="math inline">\(\phi_r\)</span>, <span class="math display">\[ \phi_r + \log\Phi^{\text{ind}}_{kr_0} - \beta n^2 f(\tfrac{r_0}{n})
        \approx \log\Phi^{\text{ind}}_{kr} - \beta n^2 f(\tfrac{r}{n}) . \]</span> We will use this approach below when looking ahead with artificial data.</p>
</div>
</div>
<div id="implem" class="section level2">
<h2 class="hasAnchor">
<a href="#implem" class="anchor"></a>Implementations</h2>
<p>Having explained the general approach, it remains to implement the computations, which consist of three parts:</p>
<ol style="list-style-type: decimal">
<li>sampling from the index constrained eigenvalue distribution,</li>
<li>reconstructing the index normalized curvature measures and the ratio of probabilities for consecutive indices from sampling data,</li>
<li>calculating the dimension normalized curvature measures.</li>
</ol>
<p>The difficulty of the first step lies again in the concentration effects. A random vector from the <span class="math inline">\(\text{G}\beta\text{E}\)</span> will have its index close to <span class="math inline">\(n/2\)</span> with high probability, making a simple rejection sampler (sample from <span class="math inline">\(\text{G}\beta\text{E}\)</span> then reject if the index is not in the required range) a practically infeasible approach. Instead we will use the Hamiltonian Monte-Carlo sampler <a href="http://mc-stan.org/">Stan</a> (<a href="https://en.wikipedia.org/wiki/Stan_(software)">wikipedia</a>) for this task.</p>
<p>The second step is solved by adapting the expectation maximization (EM) approach for reconstructing the intrinsic volumes to the situation at hand. The required changes are minimal and and described below.</p>
<p>The third step is just an implementation of the computation as explained <a href="#ind_constr_cm">above</a>.</p>
<div id="sampl_constr_eigval" class="section level3">
<h3 class="hasAnchor">
<a href="#sampl_constr_eigval" class="anchor"></a>Sampling index constrained eigenvalues</h3>
<p>The sampler <a href="http://mc-stan.org/">Stan</a> works solely through the log-likelihood function, which only has to be given up to additive constant (so the normalizing constant for the density does not have to be specified). Concretely, the density of <span class="math inline">\(x\sim\text{G}\beta\text{E}\)</span> is given by <span class="math display">\[ p(x) \propto e^{-\|x\|^2/2} \prod_{i&lt;j} \big|x_i-x_j\big|^\beta . \]</span> The corresponding log-likelihood is easily computed in Stan; the restriction on the index are realized by grouping the ordered eigenvalues into “positive”, “free”, “negative” and by requiring that the positive and negative ones do not change sign and the free eigenvalues lie between these two groups.</p>
<p>Concretely, if <span class="math inline">\(n=40\)</span> and the index shall lie between <span class="math inline">\(25\)</span> and <span class="math inline">\(35\)</span>, then we assume that the eigenvalues <span class="math inline">\(x_1\leq x_2 \leq \dots \leq x_{40}\)</span> are grouped into <span class="math inline">\(5\)</span> eigenvalues with negative sign, <span class="math inline">\(x_1,\ldots,x_5\)</span>, <span class="math inline">\(10\)</span> eigenvalues with no prescribed sign, <span class="math inline">\(x_6,\ldots,x_{15}\)</span>, and the remaining <span class="math inline">\(25\)</span> eigenvalues with positive sign, <span class="math inline">\(x_{16},\ldots,x_{40}\)</span>.</p>
<p>The function <code>constr_eigval</code> from the <code>symconivol</code> package generates the Stan model for the index constrained eigenvalue distribution. The models are slightly different for whether there are “positive”, “free”, or “negative”, eigenvalues allowed. Passing this to the Stan sampler and then running it with the data (the Dyson index <span class="math inline">\(\beta\)</span> and the numbers of positive/free/negative eigenvalues) yields samples from the resulting index constrained eigenvalue distribution.</p>
<p><strong>Example computations:</strong></p>
<p>The following lines will construct a model for negative, free, and positive eigenvalues, then run it for <span class="math inline">\(5\)</span> negative, <span class="math inline">\(10\)</span> free, and <span class="math inline">\(25\)</span> positive eigenvalues, and then extract the sampled eigenvalues:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filename &lt;-<span class="st"> "tmp.stan"</span>
M &lt;-<span class="st"> </span><span class="kw"><a href="../reference/constr_eigval.html">constr_eigval</a></span>( <span class="dt">beta=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">40</span>, <span class="dt">ind_low=</span><span class="dv">25</span>, <span class="dt">ind_upp=</span><span class="dv">35</span>, <span class="dt">filename=</span>filename )
stan_samp &lt;-<span class="st"> </span><span class="kw">stan</span>( <span class="dt">file =</span> filename, <span class="dt">data =</span> M$data, <span class="dt">chains =</span> <span class="dv">1</span>, <span class="dt">warmup =</span> <span class="fl">1e3</span>,
                   <span class="dt">iter =</span> <span class="fl">1e5</span>, <span class="dt">cores =</span> <span class="dv">2</span>, <span class="dt">refresh =</span> <span class="fl">1e4</span> )
<span class="kw">file.remove</span>(filename)
samp &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">ep =</span> rstan::<span class="kw">extract</span>(stan_samp)$ep,
              <span class="dt">ef =</span> rstan::<span class="kw">extract</span>(stan_samp)$ef,
              <span class="dt">en =</span> rstan::<span class="kw">extract</span>(stan_samp)$en )</code></pre></div>
<p>The resulting empirical eigenvalue distribution(s) look as follows:</p>
<p><img src="nn_nf_np=5_10_25.png" width="673" style="display: block; margin: auto;"></p>
<p>To illustrate the different situations we illustrate these empirical eigenvalue distributions for some more values for the parameters:</p>
<ul>
<li><strong><span class="math inline">\(0\)</span> negative, <span class="math inline">\(15\)</span> free, <span class="math inline">\(25\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=0_15_25.png" width="673" style="display: block; margin: auto;"></p>
<ul>
<li><strong><span class="math inline">\(16\)</span> negative, <span class="math inline">\(0\)</span> free, <span class="math inline">\(24\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=16_0_24.png" width="673" style="display: block; margin: auto;"></p>
<ul>
<li><strong><span class="math inline">\(35\)</span> negative, <span class="math inline">\(5\)</span> free, <span class="math inline">\(0\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=35_5_0.png" width="673" style="display: block; margin: auto;"></p>
<ul>
<li><strong><span class="math inline">\(20\)</span> negative, <span class="math inline">\(0\)</span> free, <span class="math inline">\(0\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=20_0_0.png" width="673" style="display: block; margin: auto;"></p>
<ul>
<li><strong><span class="math inline">\(0\)</span> negative, <span class="math inline">\(40\)</span> free, <span class="math inline">\(0\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=0_40_0.png" width="673" style="display: block; margin: auto;"></p>
<ul>
<li><strong><span class="math inline">\(0\)</span> negative, <span class="math inline">\(0\)</span> free, <span class="math inline">\(40\)</span> positive eigenvalues:</strong></li>
</ul>
<p><img src="nn_nf_np=0_0_40.png" width="673" style="display: block; margin: auto;"></p>
</div>
<div id="reconstr_ind" class="section level3">
<h3 class="hasAnchor">
<a href="#reconstr_ind" class="anchor"></a>Reconstructing index normalized curvature measures</h3>
<p>The weights of a bivariate chi-bar-squared distribution can be reconstructed as described in the vignette <a href="https://damelunx.github.io/conivol/articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a> from the <code>conivol</code> package. We explain again the main idea behind this algorithm in the specific context of curvature measures.</p>
<p>In the following we will assume that the index constraints are of the form <span class="math inline">\(r\leq \text{ind}(x)\leq r+s\)</span>, that is, <span class="math inline">\(R=\{r,\ldots,r+s\}\)</span>, and we assume <span class="math inline">\(0&lt; r&lt; n\)</span> and <span class="math inline">\(0\leq s&lt; n-r\)</span>. We assume strict inequalities so that we can assume that the positive and negative components of the eigenvalue vector are always nonzero (the cases of all positive or all negative eigenvalues are not interesting for the index/dimension normalized curvature measures). As explained <a href="#curv_meas">above</a>, we can turn a sample from the index constrained Gaussian orthogonal/unitary/symplectic ensemble into a sample from the bivariate chi-bar-squared distribution by taking the squared norms of the positive and negative components, <span class="math display">\[ (X,Y) = \big( \|x_+\|^2, \|x_-\|^2 \big) . \]</span> The distribution of <span class="math inline">\((X,Y)\)</span> can then be described in terms of the latent variable <span class="math inline">\(Z\)</span>, <span class="math display">\[ \text{Prob}(Z=k) =
    \frac{\sum_{j=0}^s \Phi_{k,r+j}}{\sum_{\ell=1}^{d-1} \sum_{j=0}^s
    \Phi_{\ell,r+j}} , \]</span> through the conditional distributions <span class="math display">\[ X\mid Z\sim \chi_Z^2,\quad Y\mid Z\sim \chi_{d-Z}^2 ,\quad
        X\mid Z,\; Y\mid Z \text{ independent} . \]</span></p>
<p>The Pataki bounds show that <span class="math inline">\(\sum_{j=0}^s \Phi_{k,r+j}\)</span> is nonzero iff <span class="math display">\[ r+\beta\binom{r}{2} \leq k \leq d - \Big( n-r-s + \beta \binom{n-r-s}{2} \Big) . \]</span> In particular, since we assume that <span class="math inline">\(0&lt;r&lt;n\)</span> and <span class="math inline">\(s&lt;n-r\)</span>, the latent variable <span class="math inline">\(Z\)</span> will not take the values <span class="math inline">\(0\)</span> or <span class="math inline">\(d\)</span> with positive probability. So the latent variable is indeed entirely hidden, which is different from the <a href="../../conivol/vignettes/estim-conic-intrinsic-volumes-with-EM.html">intrinsic volumes case</a>.</p>
Assuming that we have <span class="math inline">\(N\)</span> samples <span class="math inline">\((\mathbf{X},\mathbf{Y})=\big((X_1,Y_1),\ldots,(X_N,Y_N)\big)\)</span>, the likelihood function, up to normalizing constant, is given by <span class="math display">\[ L(\Phi\mid \mathbf{X},\mathbf{Y}) \propto
    \prod_{i=1}^N \sum_{k=1}^{d-1} f_{ik}\sum_{j=0}^s\Phi_{k,r+j} ,\qquad
    f_{ik} = f_k(X_i) f_{d-k}(Y_i) , \]</span> where <span class="math inline">\(f_k(x)\)</span> denotes the density of the chi-squared distribution, <span class="math inline">\(f_k(x)\propto x^{k/2-1}e^{-x/2}\)</span>. Taking the latent variable into account, we obtain
<span class="math display">\[\begin{align*}
    L(\Phi\mid \mathbf{X},\mathbf{Y},\mathbf{Z}) &amp; \propto
    \prod_{i=1}^N
    \Big( f_{iZ_i}\sum_{j=0}^s\Phi_{Z_i,r+j} \Big) ,
    \\ \log L(\Phi\mid \mathbf{X},\mathbf{Y},\mathbf{Z}) &amp; =
    \underbrace{\sum_{i=1}^N \log f_{iZ_i}}_{\text{(indep. of $\Phi$)}} +
    \sum_{i=1}^N \log\Big(\sum_{j=0}^s\Phi_{Z_i,r+j}\Big) + \text{const} .
\end{align*}\]</span>
The EM algorithm tries to find the maximum likelihood estimate of the parameters of the latent variable by maximizing the conditional likelihood function, with the likelihood of the data expressed in the latent variable, which are conditioned on the current iterate. Concretely, we express the posterior density for the <span class="math inline">\(i\)</span>th latent variable in the form
<span class="math display">\[\begin{align*}
    p\big(Z_i=k\mid X_i=x_i,Y_i=y_i\big) &amp; =
    \frac{p\big(X_i=x_i,Y_i=y_i\mid Z_i=k\big)\,p\big(Z_i=k\big)}{p(X_i=x_i,Y_i=y_i)}
\\ &amp; = \frac{f_{ik} \sum_{j=0}^s\Phi_{k,r+j}\big/\sum_{\ell=1}^{d-1}\sum_{j=0}^s\Phi_{\ell,r+j}}
            {\sum_{\ell=1}^{d-1}f_{i\ell}\sum_{j=0}^s\Phi_{\ell,r+j}
            \big/\sum_{\ell=1}^{d-1}\sum_{j=0}^s\Phi_{\ell,r+j}}
\\ &amp; = \frac{f_{ik} \sum_{j=0}^s\Phi_{k,r+j}}
            {\sum_{\ell=1}^{d-1}f_{i\ell}\sum_{j=0}^s\Phi_{\ell,r+j}} .
\end{align*}\]</span>
The EM algorithm then finds the <span class="math inline">\((t+1)\)</span>th iterate <span class="math inline">\(\Phi^{(t+1)}\)</span> from the <span class="math inline">\(t\)</span>th iterate <span class="math inline">\(\Phi^{(t)}\)</span> by maximizing the following function in <span class="math inline">\(\Phi\)</span>:
<span class="math display">\[\begin{align*}
    \underset{\mathbf{Z}\mid \mathbf{X},\mathbf{Y},\Phi^{(t)}}{\text{E}}
    \big[\log L\big(\Phi \,\big|\, \mathbf{X},\mathbf{Y},
    \mathbf{Z} \big)\big] &amp; = \sum_{i=1}^N \sum_{k=1}^{d-1}
    \frac{f_{ik} \sum_{j=0}^s\Phi_{k,r+j}^{(t)}}
            {\sum_{\ell=1}^{d-1}f_{i\ell}\sum_{j=0}^s\Phi_{\ell,r+j}^{(t)}}
    \log\Big(\sum_{j=0}^s\Phi_{k,r+j}\Big)
    + \text{const} .
\end{align*}\]</span>
<p>This function, which we divide by the number of samples to achieve a convergence of the constants, <span class="math display">\[ F(a_1,\ldots,a_{d-1}) =
    \sum_{k=1}^{d-1} \bigg( \frac{1}{N} \sum_{i=1}^N
    \frac{f_{ik} \sum_{j=0}^s\Phi_{k,r+j}^{(t)}}
            {\sum_{\ell=1}^{d-1}f_{i\ell}\sum_{j=0}^s\Phi_{\ell,r+j}^{(t)}}\bigg)
    \log a_k , \]</span> can be maximized over the probability simplex as a separable convex program. In the <code>symconivol</code> package this is achieved via <a href="https://www.mosek.com/">MOSEK</a>.</p>
<p>So this will be one step of the EM algorithm.</p>
<p><strong>Log-concavity:</strong> In the case of intrinsic volumes it has turned out that assuming log-concavity is essential for obtaining a stable convergence of the EM algorithm. Recall that a sequence <span class="math inline">\(a_0,a_1,\ldots,a_d\)</span> of positive numbers is log-concave if <span class="math display">\[ \log a_k \geq \frac{\log a_{k-1} + \log a_{k+1}}{2} . \]</span> These inequalities can be easily enforced in the EM algorithm by projecting the vector of the logarithm onto the convex cone formed by these sequences.</p>
<p>In the case of intrinsic volumes the validity of these inequalities is a well-established conjecture. In the case of the curvature measures this is less well tested. Nevertheless, we will enforce log-concavity when reconstructing the curvature measures as this will stabilize the EM algorithm, and because it seems to be a reasonable assumption that improves the whole procedure work.</p>
<p><strong>Example computations:</strong></p>
<p>We evaluate the index constrained eigenvalue data from above with <span class="math display">\[ \beta=1 ,\; n=40 ,\; r=25 ,\; s=10 . \]</span> Let’s have a look at the Pataki bounds for these values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span><span class="dv">1</span>
n &lt;-<span class="st"> </span><span class="dv">40</span>
r &lt;-<span class="st"> </span><span class="dv">25</span>
s &lt;-<span class="st"> </span><span class="dv">10</span>
pat &lt;-<span class="st"> </span><span class="kw"><a href="../reference/pat_bnd.html">pat_bnd</a></span>(beta,n)
<span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">"dimension: d="</span>, pat$d))
<span class="co">#&gt; [1] "dimension: d=820"</span>
<span class="kw">print</span>(<span class="kw">str_c</span>(<span class="st">"Pataki bounds: "</span>,pat$<span class="kw">k_low</span>(r),<span class="st">", "</span>,pat$<span class="kw">k_upp</span>(r+s)))
<span class="co">#&gt; [1] "Pataki bounds: 325, 805"</span></code></pre></div>
<p>The following lines transform the eigenvalue data first into bivariate chi-bar-squared data, then evaluate the densities for the EM algorithm, and then run the EM algorithm for <span class="math inline">\(100\)</span> iterations. The list <code>samp</code> was obtained in the <a href="#sampl_constr_eigval">previous section</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert sample data to bivariate chi-bar-squared data:</span>
m_samp &lt;-<span class="st"> </span><span class="kw"><a href="../reference/constr_eigval_to_bcbsq.html">constr_eigval_to_bcbsq</a></span>(samp)

<span class="co"># prepare data for EM algorithm:</span>
data &lt;-<span class="st"> </span><span class="kw"><a href="../reference/prepare_em_cm.html">prepare_em_cm</a></span>(<span class="dt">d=</span>pat$d, <span class="dt">low=</span>pat$<span class="kw">k_low</span>(r), <span class="dt">upp=</span>pat$<span class="kw">k_upp</span>(r+s), <span class="dt">m_samp=</span>m_samp)

<span class="co"># run EM algorithm for 100 steps:</span>
em &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em_cm.html">estim_em_cm</a></span>(<span class="dt">d=</span>pat$d, <span class="dt">low=</span>pat$<span class="kw">k_low</span>(r), <span class="dt">upp=</span>pat$<span class="kw">k_upp</span>(r+s), <span class="dt">m_samp=</span>m_samp, <span class="dt">N=</span><span class="dv">100</span>,
                  <span class="dt">data=</span>data, <span class="dt">no_of_lcc_projections =</span> <span class="dv">1</span>)

<span class="co"># display some iterates of the EM algorithm</span>
tib_plot &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em[<span class="dv">1+10</span>*(<span class="dv">0</span>:<span class="dv">10</span>), ]) ) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span>pat$<span class="kw">k_low</span>(r):pat$<span class="kw">k_upp</span>(r+s),<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">12</span>)
G &lt;-<span class="st"> </span><span class="kw">ggplot</span>(tib_plot,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">"none"</span>, <span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p>The resulting plot looks as follows:</p>
<p><img src="ind_norm_from_constr_eigval.png" width="673" style="display: block; margin: auto;"></p>
<p>We can use this method to shed some light on the Pataki range, as displayed above for <span class="math inline">\(n=3,6,10\)</span>. For the index normalized curvature measures we obtain the following picture:</p>
<p><img src="curv-meas_figures/illustr-ind-norm-cm-1.png" width="672" style="display: block; margin: auto;"></p>
<p>The dashed curves indicate the asymptotic Patali range and <em>Leigh’s curve</em>, which we describe <a href="#leighscurve">below</a> in more details. Notice the apparent convergence of the index constrained curvature measures to this curve.</p>
</div>
<div id="reconstr_dim" class="section level3">
<h3 class="hasAnchor">
<a href="#reconstr_dim" class="anchor"></a>Reconstructing dimension normalized curvature measures</h3>
<p>For the reconstruction of the dimension normalized curvature measures we proceed as described <a href="#ind_constr_cm">above</a>.</p>
<p><strong>Example computations:</strong></p>
<p>We reconstruct the same range again as above, <span class="math inline">\(n=3,6,10\)</span>:</p>
<p><img src="curv-meas_figures/illustr-dim-norm-cm-1.png" width="672" style="display: block; margin: auto;"></p>
<p>We observe again a concentration behavior, but with a limit curve which is apparently different from the limit curve of the index constrained curvature measures. The indicated curve is a guess for this limit curve, based on Leigh’s curve and the rate function of the index probabilities. See <a href="#Phidimlim">below</a> for details on how we derived this curve (it should be noted that this curve is based on a rather broad assumption, and an “eye-balled” value for an essential parameter defining the curve). This curve is interesting because it can be used to predict the rank of the solution of semidefinite programs (see <a href="#appl_SDP">below</a>).</p>
</div>
</div>
<div id="look_ahead" class="section level2">
<h2 class="hasAnchor">
<a href="#look_ahead" class="anchor"></a>Looking ahead</h2>
<p>In this sectioin we try to get a better insight in the higher dimensional behavior of the dimension normalized curvature measures. We will do this by using known and conjectured assumptions about the general behavior of the curvature measures. More precisely, while it is known (see <span class="citation">(Goldstein, Nourdin, and Peccati <a href="#ref-GNP17">2017</a>)</span>) that the intrinsic volumes of the semidefinite cones follow a central limit theorem, that is, they approximate the normal distribution in high dimensions, this is so far not known to hold for the index normalized curvature measures. It is nevertheless a reasonable assumption to make that they, too, obey a central limit law. We will make this assumption in this section to get an idea about what happens in higher dimensions for the dimension normalized curvature measures.</p>
<div id="rate_fct" class="section level3">
<h3 class="hasAnchor">
<a href="#rate_fct" class="anchor"></a>The rate function of the index probabilities</h3>
<p>In <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span> it was shown that <span class="math display">\[ \log\text{Prob}\{\text{ind}(x)=r\} \approx
    -\beta n^2 R(\tfrac{r}{n}) + \text{const}(d) , \]</span> for some rate function <span class="math inline">\(R\)</span>, for which they give an explicit formula (see <a href="#ratefct">below</a> for details on the calculation of <span class="math inline">\(R\)</span>). The graphs of the rate function <span class="math inline">\(R\)</span> and its derivative look as follows:</p>
<p><img src="curv-meas_figures/rate-fct-1.png" width="672" style="display: block; margin: auto;"></p>
Using this, we can approximate the probability ratio as follows:
<span class="math display">\[\begin{align*}
     \log\Big(\frac{\text{Prob}\{\text{ind}(x)=r+1\}}{\text{Prob}\{\text{ind}(x)=r\}}\Big)
   &amp; \approx -2d \big( R(\tfrac{r+1}{n})-R(\tfrac{r}{n})\big)
     \approx \frac{-2d}{n}\, R'(\tfrac{r}{n}) .
\end{align*}\]</span>
<p>(We use <span class="math inline">\(2d\)</span> instead of <span class="math inline">\(\beta n^2\)</span>, as this visibly improves the approximations below.) The following plots compare the empirically estimated log of the ratio of probabilities for neighboring indices with the approximate asymptotic expressions (the estimate through the derivative of the rate function is depicted as the red dashed curve):</p>
<p><img src="curv-meas_figures/approx-indprobrat-1.png" width="672" style="display: block; margin: auto;"></p>
<!-- Replacing the empirically estimated log of ratio of index probabilities with the -->
<!-- asymptotic expressions yields to the following reconstructed dimension -->
<!-- normalized curvature measures: -->
<!-- ```{r illustr-dim-norm-cm_ratefct, fig.width = 7, fig.height=12, echo=FALSE} -->
<!-- plotsPB <- list() -->
<!-- plotPhiDim <- list() -->
<!-- j <- 0 -->
<!-- load("../data/indprob.RData") -->
<!-- R <- rate() -->
<!-- for (beta in c(1,2,4)) { -->
<!--     for (n in c(3,6,10)) { -->
<!--         j <- j+1 -->
<!--         pat <- pat_bnd(beta,n) -->
<!--         d <- pat$d -->
<!--         IndP <- matrix(0,2,n-2) -->
<!--         for (r in 1:(n-2)) { -->
<!--             if (r==(n-1)/2) { -->
<!--                 IndP[,r] <- c(1,1) -->
<!--             } else if (r<(n-1)/2) { -->
<!--                 index <- str_c("beta=",beta,",n=",n,",r=",n-r-1) -->
<!--                 IndP[,r] <-indprob[[index]][c(2,1)] -->
<!--             } else { -->
<!--                 index <- str_c("beta=",beta,",n=",n,",r=",r) -->
<!--                 IndP[,r] <-indprob[[index]] -->
<!--             } -->
<!--         } -->
<!--         B <- plotPhiInd[[j]] -->
<!--         A <- matrix(0,d+1,n+1) -->
<!--         A[1,1] <- 1 -->
<!--         A[d+1,n+1] <- 1 -->
<!--         for (k in 1:(d-1)) { -->
<!--             rmin <- pat$r_low(k) -->
<!--             rmax <- pat$r_upp(k) -->
<!--             logR <- log(B[1+k,1+rmin:rmax])-2*d*R$lkup_R((rmin:rmax)/n) -->
<!--             A[1+k,1+rmin:rmax] <- exp(logR)/max(exp(logR)) -->
<!--         } -->
<!--         plotPhiDim[[j]] <- A -->
<!--         plotsPB[[j]] <- plot_Phi(A, TRUE, FALSE, TRUE) -->
<!--     } -->
<!-- } -->
<!-- multiplot(plotlist = plotsPB, cols = 3) -->
<!-- ``` -->
</div>
<div id="var_ivol" class="section level3">
<h3 class="hasAnchor">
<a href="#var_ivol" class="anchor"></a>The variance of the intrinsic volumes</h3>
<p>Before considering the curvature measures, we mention that the variance of the intrinsic volumes <span class="math inline">\(v=(\sum_r \Phi_{kr})\)</span> have a variance of approximating <span class="math inline">\(d\big(\frac{8}{\pi^2}-\frac{1}{2}\big)\)</span>. More precisely, this was shown in <span class="citation">(McCoy and Tropp <a href="#ref-McCT14">2014</a>)</span> for <span class="math inline">\(\beta=1\)</span>, but as the following estimates using the bivariate chi-bar-squared distribution suggest, the formula should also hold for <span class="math inline">\(\beta=2,4\)</span>.</p>
<p><img src="var_ivol.png" width="673" style="display: block; margin: auto;"></p>
</div>
<div id="the-variance-of-index-normalized-curvature-measures" class="section level3">
<h3 class="hasAnchor">
<a href="#the-variance-of-index-normalized-curvature-measures" class="anchor"></a>The variance of index normalized curvature measures</h3>
<p>So far, no asymptotic estimates for the variance of the index normalized curvature measures are known. We obtain the following plots for the estimated values:</p>
<p><img src="curv-meas_figures/est-var-PhiInd-1.png" width="672" style="display: block; margin: auto;"></p>
<p>The above plots show the normalized empirical variance of <span class="math inline">\(\Phi^{\text{ind}}_{kr}\)</span>, that is, the curves are given by <span class="math display">\[ \bigg( \frac{r}{n} ,\, \frac{\sum_k k^2 \Phi^{\text{ind}}_{kr} -
        \big(\sum_k k \Phi^{\text{ind}}_{kr}\big)^2}{d} \bigg) \qquad
        \Big[ \text{in gray:}\quad \big(\rho,\,0.2\big(1-\big|2\rho-1\big|\big)\big)
        \Big] . \]</span> The constant <span class="math inline">\(0.2\)</span> is eye-balled from the plots, and probably incorrect. But it seems justified to assume that the variance of the index constrained curvature measures can be approximated in the form <span class="math display">\[ C\,d\,\big(1-\big|\tfrac{2r}{n}-1\big|\big) , \]</span> for some constant <span class="math inline">\(C\)</span> (<span class="math inline">\(\approx 0.2\)</span>).</p>
</div>
<div id="rec_art" class="section level3">
<h3 class="hasAnchor">
<a href="#rec_art" class="anchor"></a>Reconstructing dimension normalized curvature measures from artificial data</h3>
<p>Combining the assumption of a central limit law for the index normalized curvature measures with the above guess for their variance, we obtain a guesstimate of <span class="math display">\[ \log \Phi^{\text{ind}}_{kr} \approx - d
  \frac{\big(\frac{k}{d}-\ell(\frac{r}{n})\big)^2}{2C\big(1-\big|\tfrac{2r}{n}-1\big|\big)} , \]</span> where <span class="math inline">\(\ell(\rho)\)</span> denotes Leigh’s curve (see <a href="#leighscurve">below</a> for details), and where we use the estimate <span class="math inline">\(C=0.2\)</span>. We can renormalize these using the rate function of the index probabilities, <span class="math display">\[ \log\text{Prob}\{\text{ind}(x)=r\} \approx -2d\, R(\tfrac{r}{n}) + \text{const}(d) , \]</span> which results in the following pictures for the estimated dimension normalized curvature measures:</p>
<p><img src="curv-meas_figures/illustr-dim-norm-cm_artif-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="Phidimlim" class="section level3">
<h3 class="hasAnchor">
<a href="#Phidimlim" class="anchor"></a>Computing the limit curve (with lots of assumptions)</h3>
<p>We can compute the limit curve for the dimension normalized curvature measures in terms of the estimated constant <span class="math inline">\(C\)</span>, and of course relying on the two main assumptions that</p>
<ol style="list-style-type: decimal">
<li>the index normalized curvature measures obey a central limit law,</li>
<li>the variance of the index normalized curvature measures has the form <span class="math inline">\(C\big(1-\big|\frac{2r}{n}-1\big|\big)\)</span>.</li>
</ol>
Combining this assumptions and estimates yields
<span class="math display">\[\begin{align*}
    \log\Phi_{kr} &amp; = \log\Phi^{\text{ind}}_{kr} + \log\text{Prob}\{\text{ind}(x)=r\}
\\ &amp; \approx - d
  \frac{\big(\frac{k}{d}-\ell(\frac{r}{n})\big)^2}
  {2C\big(1-\big|\tfrac{2r}{n}-1\big|\big)} -2d\, R(\tfrac{r}{n}) + \text{const}(d) .
\end{align*}\]</span>
<p>The limit curve of the dimension normalized curvature measures (found through their modes) is given by the maximum of this function in <span class="math inline">\(r\)</span>. Substituting <span class="math inline">\(\rho:=\frac{r}{n}\)</span> and <span class="math inline">\(\kappa:=\frac{k}{d}\)</span>, we see that the curve is described through the minimum (in <span class="math inline">\(\rho\)</span>) of <span class="math display">\[ \frac{(\kappa-\ell(\rho))^2}{1-|2\rho-1|} + 4C\, R(\rho) ,\quad
        1-\sqrt(1-\kappa) \leq \rho \leq \sqrt(\kappa) . \]</span> We can find these minima via a simple table lookup procedure. The resulting curves (for different values of <span class="math inline">\(C\)</span>) look as follows:</p>
<p><img src="lim_PhiDim.png" width="673" style="display: block; margin: auto;"></p>
</div>
</div>
<div id="asym_indnorm" class="section level2">
<h2 class="hasAnchor">
<a href="#asym_indnorm" class="anchor"></a>Deriving the asymptotics for index constrained curvature measures</h2>
<p>In this section we provide more details about the rate function and Leigh’s curve. These are given and derived from <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span>, whose formulas we reproduce and illustrate here.</p>
<div id="semicirc" class="section level3">
<h3 class="hasAnchor">
<a href="#semicirc" class="anchor"></a>Semi-circle</h3>
<p>It is well known that the empirical distribution function of the eigenvalues, <span class="math display">\[ \frac{1}{n} \sum_{j=1}^n\delta_{x_j}(t) ,\quad
    (x_1,\ldots,x_n) \sim \text{G}\beta\text{E} , \]</span> approximates a <a href="https://en.wikipedia.org/wiki/Wigner_semicircle_distribution">semi-circle distribution</a> in high dimensions. More precisely, in order to obtain this limit one has to rescale the eigenvalues by <span class="math inline">\(1/\sqrt{n}\)</span>, and in order to obtain the limit distribution independent of the Dyson index, one additionally rescales by by <span class="math inline">\(1/\sqrt{\beta}\)</span>: <span class="math display">\[ g(t) = \frac{1}{n} \sum_{j=1}^n\delta_{y_j}(t) ,\quad
    y=\frac{x}{\sqrt{\beta n}} ,\quad x \sim \text{G}\beta\text{E} , \]</span> converges almost surely to the semicircle distribution with support <span class="math inline">\([-\sqrt{2},\sqrt{2}]\)</span>. Before modifying this model to the case of a fixed index ratio, let us have a look at some example cases to illustrate this wonderful classical result.</p>
<p><strong>Example computations:</strong></p>
<p>We find the empirical distribution through iterated sampling (sample size <span class="math inline">\(10^5\)</span>). For <span class="math inline">\(n=10\)</span> we obtain the following picture:</p>
<p><img src="semic_n=10.png" width="673" style="display: block; margin: auto;"></p>
<p>For <span class="math inline">\(n=40\)</span> the convergence becomes evident:</p>
<p><img src="semic_n=40.png" width="673" style="display: block; margin: auto;"></p>
</div>
<div id="limdist_indfct" class="section level3">
<h3 class="hasAnchor">
<a href="#limdist_indfct" class="anchor"></a>Limit distribution for fixed index ratio</h3>
<p>The case of a fixed index ratio was considered in <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span>; they considered the empirical distribution function <span class="math display">\[ g_r(t) = \frac{1}{n} \sum_{j=1}^n\delta_{y_j}(t) ,\quad
    y=\frac{x}{\sqrt{\beta n}} ,\quad x \sim \text{G}\beta\text{E}\mid\text{ind}=r , \]</span> and found the limit distribution for the case <span class="math inline">\(\frac{r}{n}\to \rho\)</span>. (The semicircle is found in the case <span class="math inline">\(\rho=1/2\)</span>.)</p>
<p>Denoting the limit distribution by <span class="math inline">\(g_\rho^*\)</span>, by symmetry, we have <span class="math inline">\(g_\rho^*(t)=g_{1-\rho}^*(-t)\)</span>, and for <span class="math inline">\(\frac{1}{2}\leq \rho\leq 1\)</span> the distribution <span class="math inline">\(g_\rho^*\)</span> can be found as follows:</p>
<ul>
<li>find the parameter <span class="math inline">\(b\in[0,1]\)</span> by inverting the following function:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="math display">\[ \rho = \frac{L^2}{\pi(1+b)}
\int_0^1 \sqrt{\frac{(1-t)\;(1+t(1+b))\;(b+t(1+b))}{t}}\,dt , \]</span> where <span class="math display">\[ L = \frac{\sqrt{2}\;(1+b)}{\sqrt{1+b+b^2}} , \]</span>
</li>
</ul>
<p><img src="curv-meas_figures/illustr_params_MNSV-1.png" width="672" style="display: block; margin: auto;"></p>
<ul>
<li>the support of <span class="math inline">\(g_\rho^*\)</span> is then given by <span class="math display">\[ \frac{L}{1+b}\,[-1,-b] \cup L\,[0,1] , \]</span> and the function values within these intervals are given by <span class="math display">\[ g_\rho^*(L\,t) = \frac{L}{\pi(1+b)}\sqrt{\frac{(1-t)\;(1+t(1+b))\;(b+t(1+b))}{t}} . \]</span>
</li>
</ul>
<p><img src="curv-meas_figures/lim_dens_MNSV-1.png" width="672" style="display: block; margin: auto;"></p>
The integral expression of <span class="math inline">\(\rho\in[\frac{1}{2},1]\)</span> in terms of <span class="math inline">\(b\in[0,1]\)</span> can be solved in terms of special functions:
<span class="math display">\[\begin{align*}
    \rho &amp; = \frac{2b}{\pi\sqrt{1+2b}} \bigg( \Pi\Big(\frac{1+b}{1+2b},\sqrt{\frac{1-b^2}{1+2b}}\Big)
        + \frac{(1+2b)(1-b)}{2(1+b+b^2)} K\Big(\sqrt{\frac{1-b^2}{1+2b}}\Big) \bigg) ,
\end{align*}\]</span>
where <span class="math inline">\(K\)</span> and <span class="math inline">\(\Pi\)</span> denote the <a href="https://en.wikipedia.org/wiki/Legendre_form">Legendre form</a> of the <a href="https://dlmf.nist.gov/19.2">complete elliptic integral of first and third kind</a>,
<span class="math display">\[\begin{align*}
    K(k) &amp; = \int_0^1 \frac{dt}{\sqrt{1-t^2}\sqrt{1-k^2t^2}} ,
\\ \Pi(\alpha^2,k) &amp; = \int_0^1 \frac{dt}{\sqrt{1-t^2}\sqrt{1-k^2t^2}(1-\alpha^2t^2)} .
\end{align*}\]</span>
For <a href="#leighscurve">later use</a> we also mention the complete elliptic integral of second kind,
<span class="math display">\[\begin{align*}
    E(k) &amp; = \int_0^1 \sqrt{\frac{1-k^2t^2}{1-t^2}}\,dt .
\end{align*}\]</span>
<p><strong>Example computations:</strong></p>
<p>We illustrate the asymptotics for the fixed ratio case as in the case of the semicircle. For <span class="math inline">\(\rho=0.6\)</span> we obtain for <span class="math inline">\(n=10\)</span>: <img src="indlim_n_cPerc=10_60.png" width="673" style="display: block; margin: auto;"></p>
<p>For <span class="math inline">\(n=40\)</span> the convergence becomes evident: <img src="indlim_n_cPerc=40_60.png" width="673" style="display: block; margin: auto;"></p>
<p>Similarly, for <span class="math inline">\(\rho=0.8\)</span> we obtain for <span class="math inline">\(n=10\)</span>: <img src="indlim_n_cPerc=10_80.png" width="673" style="display: block; margin: auto;"></p>
<p>For <span class="math inline">\(n=40\)</span> we obtain the following picture: <img src="indlim_n_cPerc=40_80.png" width="673" style="display: block; margin: auto;"></p>
</div>
<div id="ratefct" class="section level3">
<h3 class="hasAnchor">
<a href="#ratefct" class="anchor"></a>Calculating the rate function</h3>
In <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span> (equation (92)) the following formula for the rate function <span class="math inline">\(R(\rho)\)</span> (for <span class="math inline">\(\rho&gt;\frac{1}{2}\)</span>; <span class="math inline">\(R(\rho)=R(1-\rho)\)</span>) has been given:
<span class="math display">\[\begin{align*}
    &amp; R(\rho) = \frac{L^2-1-\log(2L^2)}{4} + \frac{\rho}{2} \int_1^{\infty}
     tL^2-\frac{1}{t}-\frac{L^2}{1+b}\sqrt{\frac{(t-1)\;(1+t(1+b))\;(b+t(1+b))}{t}}\,dt
\\ &amp; + \frac{1-\rho}{2} \Big(
    \log(a) - \frac{b(2+b)}{1+b+b^2}
    + \int_{1/(1+b)}^{\infty}
     tL^2-\frac{1}{t}-\frac{L^2}{1+b}\sqrt{\frac{(1+t)\;(t(1+b)-1)\;(t(1+b)-b)}{t}}\,dt \Big) ,
\end{align*}\]</span>
<p>with <span class="math inline">\(b\)</span> and <span class="math inline">\(L\)</span> as defined <a href="#limdist_indfct">above</a>. The rate function <span class="math inline">\(R(\rho)\)</span> is accessible via <code><a href="../reference/rate.html">rate()</a></code> from the <code>symconivol</code> package. See <a href="#rate_fct">above</a> for plots of <span class="math inline">\(R(\rho)\)</span> and its derivative.</p>
</div>
<div id="leighscurve" class="section level3">
<h3 class="hasAnchor">
<a href="#leighscurve" class="anchor"></a>Limit curve for index normalized curvature measures</h3>
Recall the notation for the empirical distribution function in the index constrained case: <span class="math display">\[ g_r(t) = \frac{1}{n} \sum_{j=1}^n\delta_{y_j}(t) ,\quad
    y=\frac{x}{\sqrt{\beta n}} ,\quad x \sim \text{G}\beta\text{E}\mid\text{ind}=r . \]</span> Similarly, we define the empirical distribution function for fixed <span class="math inline">\(x\)</span>, <span class="math display">\[ g(x;t) = \frac{1}{n} \sum_{j=1}^n\delta_{y_j}(t) ,\quad
    y=\frac{x}{\sqrt{\beta n}} , \]</span> so that <span class="math display">\[ g_r(t) = g(x;t) \quad \text{if } x \sim \text{G}\beta\text{E}\mid\text{ind}=r . \]</span> We can use this function to express the squared norm of the positive part of <span class="math inline">\(x\)</span>: <span class="math display">\[ \|x_+\|^2 = n\int_0^\infty \big(\sqrt{\beta n}\,t\big)^2 g(x;t) dt
  = \beta n^2\int_0^\infty t^2 g(x;t) dt , \]</span> and similarly the squared norm of the negative part of <span class="math inline">\(x\)</span>, <span class="math inline">\(\|x_-\|^2=\beta n^2\int_{-\infty}^0 t^2 g(x;t) dt\)</span>. The convergence of the distribution function implies<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> the concentration of <span class="math inline">\(\frac{\|x_+\|^2}{d}\)</span> (recall that <span class="math inline">\(d\approx\beta n^2/2\)</span>) around its mean, <span class="math display">\[ \text{E}\Big[\frac{\|x_+\|^2}{d}\Big] = \frac{\beta n^2}{d}
  \int_0^\infty t^2 g_r(t) dt , \]</span> which itself converges, for <span class="math inline">\(n\to\infty\)</span>, to
<span class="math display">\[\begin{align*}
    \ell(\rho) &amp; = 2 \int_0^\infty s^2 g_\rho^*(s)\,ds
\\ &amp; = \frac{2L}{\pi(1+b)} \int_0^L s^2 \sqrt{\frac{(1-s/L)\;(1+s/L(1+b))\;(b+s/L(1+b))}{s/L}}\,ds
\\ &amp; = \frac{2L^4}{\pi(1+b)} \int_0^1 \sqrt{t^3\;(1-t)\;(1+t(1+b))\;(b+t(1+b))}\,dt ,
\end{align*}\]</span>
with <span class="math inline">\(b\)</span> and <span class="math inline">\(L\)</span> as defined <a href="#limdist_indfct">above</a>. We call the curve <span class="math inline">\(\ell\colon[0,1]\to[0,1]\)</span> <em>Leigh’s curve</em>. It is accessible via <code><a href="../reference/leigh.html">leigh()</a></code> from the <code>symconivol</code> package. It can be expressed in terms of elliptic integrals in the following way:
<span class="math display">\[\begin{align*}
    \ell(\rho) &amp; = \rho + \frac{3b(1+b)\sqrt{1+2b}}{\pi(1+b+b^2)^2} \bigg(
    E\Big(\sqrt{\frac{1-b^2}{1+2b}}\Big)
    - \frac{2+b}{3} K\Big(\sqrt{\frac{1-b^2}{1+2b}}\Big) \bigg) .
\end{align*}\]</span>
<p>Here is a plot of this curve:</p>
<p><img src="curv-meas_figures/leighscurve-1.png" width="672" style="display: block; margin: auto;"></p>
<p>The dashed curves indicate as usual the (asymptotic) Pataki range. We conjecture, and it seems apparent from the example computations <a href="#reconstr_ind">above</a>, that the index normalized curvature measures converge to Leigh’s curve, <span class="math display">\[ \lim_{n\to\infty} \sum_{k\leq \kappa d}\Phi^{\text{ind}}_{k,\text{round}(\rho n)}
  \stackrel{(?)}{=} \begin{cases}
        0 &amp; \text{if } \kappa&lt;\ell(\rho)
    \\  1 &amp; \text{if } \kappa&gt;\ell(\rho) .
    \end{cases} \]</span> Similarly, denoting the curve that we derived <a href="#Phidimlim">above</a> for the dimension normalized curvature measures (relying on some assumptions about the higher dimensional behavior of the curvature measures) by <span class="math inline">\(\mu(\kappa)\)</span>, that is, <span class="math display">\[ \mu(\kappa) = \text{argmin}\Big\{
        \frac{(\kappa-\ell(\rho))^2}{1-|2\rho-1|} + 4C\, R(\rho) \,\Big|\;
        1-\sqrt{1-\kappa} \leq \rho \leq \sqrt{\kappa} \Big\} , \]</span> we conjecture that the dimension normalized curvature measures converge to this curve, <span class="math display">\[ \lim_{n\to\infty} \sum_{r\leq \rho n}\Phi^{\text{dim}}_{\text{round}(\kappa d),r}
  \stackrel{(?)}{=} \begin{cases}
        0 &amp; \text{if } \rho&lt;\mu(\kappa)
    \\  1 &amp; \text{if } \rho&gt;\mu(\kappa) .
    \end{cases} \]</span></p>
</div>
</div>
<div id="appl_SDP" class="section level2">
<h2 class="hasAnchor">
<a href="#appl_SDP" class="anchor"></a>Application in semidefinite programming</h2>
<p>Generally speaking, a conic program with reference cone <span class="math inline">\(\mathcal{C}\)</span> is maximizing (or minimizing) a linear functional over the intersection of the (closed convex) cone <span class="math inline">\(\mathcal{C}\)</span> with an affine linear subspace of codimension <span class="math inline">\(k\)</span> (generically, the subspace is given by <span class="math inline">\(k\)</span> linear equations). A natural random model for this setup is to assume that</p>
<ol style="list-style-type: decimal">
<li>the linear functional is generic in that is is nonzero, and random in that its direction is uniformly random in the unit sphere,</li>
<li>the affine linear subspace is generic in that it does not contain the origin, and random in that its associated linear subspace (which goes through the origin) is uniformly random in the set of all linear subspaces with the same dimension (Grassmann manifold).</li>
</ol>
<p>Of course, as is the case for every random model used for analyzing an algorithm, there are complaints about the applicability of the model to “real-world” situations. But the described model is a natural model to start with, and once this is understood one might think of extending it (maybe there is a “universality law”; or if there isn’t, one might investigate what factors are key for influencing the model to show a different behavior).</p>
In <span class="citation">(Amelunxen and Bürgisser <a href="#ref-AB15s">2015</a>)</span> it was shown that the probabilities for the random conic program being infeasible or unbounded are given in terms of the intrinsic volumes of the cone:
<span class="math display">\[\begin{align*}
    \text{Prob}\{\text{program is infeasible}\} &amp; \sum_{j=0}^{k-1} v_j(\mathcal{C}) ,
\\  \text{Prob}\{\text{program is unbounded}\}  &amp; \sum_{j=k+1}^d v_j(\mathcal{C}) .
\end{align*}\]</span>
<p>Now, this result may add to the feeling that the random model is useless for real-world applications, because it implies that with high probability a random program is either infeasible or unbounded. But this conclusion is premature, as one way out of this dilemma is simply conditioning the random model to the conic program being neither infeasible nor unbounded. The same arguments as above can be used for defending this adjusted random model: it is arguably the most natural model to start with, and one may think of extending it, once its behavior is fully understood.</p>
<p>The random conic program conditioned on being neither infeasible nor unbounded almost surely has a unique solution, which we denote by <span class="math inline">\(S\)</span>. If we now assume that the reference cone <span class="math inline">\(\mathcal{C}\)</span> is the curve of positive semidefinite matrices, then one may ask about the rank of the solution <span class="math inline">\(S\)</span>. These rank probabilities are given by the dimension normalized curvature measures: <span class="math display">\[ \text{Prob}\{\text{rank}(S)=r\} = \Phi^{\text{dim}}_{rk} . \]</span> With the above conjecture about the high-dimensional behavior of the dimension normalized curvature measures in mind, we conclude that one might use the limit curve <span class="math inline">\(\mu(\kappa)\)</span> to predict the rank of the solution of a random semidefinite program: <span class="math display">\[ \text{rank}(S) \approx \mu\big(\tfrac{k}{d}\big)\,n . \]</span></p>
</div>
<div id="alg_deg" class="section level2">
<h2 class="hasAnchor">
<a href="#alg_deg" class="anchor"></a>Appendix: Comparing curvature measures and algebraic degree</h2>
<p>Another parameter associated to the semidefinite cone, with <span class="math inline">\(\beta=1\)</span>, and satisfying the Pataki inequalities is the algebraic degree of semidefinite programming <span class="math inline">\(\delta(m,n,r)\)</span>, defined in <span class="citation">(Nie, Ranestad, and Sturmfels <a href="#ref-NRS10">2010</a>)</span>. (It would be interesting to have some theory about the cases <span class="math inline">\(\beta=2,4\)</span>, but so far this seems to have not been addressed.) More precisely, <span class="math inline">\(\delta(m,n,r)\)</span> with <span class="math inline">\(1\leq m\leq n-1\)</span> is nonzero only if <span class="math display">\[ r+\binom{r}{2} \leq d-m \leq d-\Big(n-r+\binom{n-r}{2}\Big) . \]</span> Some values of the algebraic degree (for <span class="math inline">\(n\leq 14\)</span>) are accessible via the function <code><a href="../reference/alg_deg.html">alg_deg()</a></code> from the <code>symconivol</code> package. The values were computed with the formula for <span class="math inline">\(\delta(m,n,r)\)</span> given in <span class="citation">(Hiep <a href="#ref-H16">2016</a>)</span>.</p>
<p>So far no connection between the curvature measures and the algebraic degree is known that goes beyond these inequalities, but empirically one may observe the same asymptotic behavior as the curvature measures. We illustrate this by looking at the normalized marginals <span class="math display">\[ \frac{\sum_m\delta(m,n,r)}{\sum_{r,m}\delta(m,n,r)} ,\quad
       \frac{\sum_r\delta(m,n,r)}{\sum_{r,m}\delta(m,n,r)} , \]</span> which we compare with the asymptotic limits of the index probabilities <span class="math inline">\(\text{Prob}(\text{ind}(r))\)</span> and of the intrinsic volumes, respectively. That is, we compare the normalized marginals with the (normalized) rate <span class="math inline">\(\exp(-n^2R(r/n))\)</span> and with the normal distribution with mean <span class="math inline">\(d/2\)</span> and variance <span class="math inline">\(d\big(\frac{8}{\pi^2}-\frac{1}{2}\big)\)</span>, respectively:</p>
<p><img src="curv-meas_figures/illustr-marg-ad-1.png" width="672" style="display: block; margin: auto;"></p>
<p>In the same way we compare the two normalized measures <span class="math display">\[ \frac{\delta(d-k,n,r)}{\sum_j\delta(d-j,n,r)} ,\quad
       \frac{\delta(d-k,n,r)}{\sum_s\delta(d-k,n,s)} , \]</span> where we use the parameter <span class="math inline">\(k=d-m\)</span> for a better comparison with the above plots for the normalized curvature measures.</p>
<p><img src="curv-meas_figures/illustr-norm-ad-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="refs" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#refs" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-AB15s">
<p>Amelunxen, Dennis, and Peter Bürgisser. 2015. “Intrinsic Volumes of Symmetric Cones and Applications in Convex Programming.” <em>Math. Program.</em> 149 (1-2, Ser. A): 105–30. <a href="https://doi.org/10.1007/s10107-013-0740-2" class="uri">https://doi.org/10.1007/s10107-013-0740-2</a>.</p>
</div>
<div id="ref-GNP17">
<p>Goldstein, Larry, Ivan Nourdin, and Giovanni Peccati. 2017. “Gaussian Phase Transitions and Conic Intrinsic Volumes: Steining the Steiner Formula.” <em>Ann. Appl. Probab.</em> 27 (1): 1–47. doi:<a href="https://doi.org/10.1214/16-AAP1195">10.1214/16-AAP1195</a>.</p>
</div>
<div id="ref-H16">
<p>Hiep, Dang Tuan. 2016. “A Formula for the Algebraic Degree in Semidefinite Programming.” <em>Kodai Math. J.</em> 39 (3): 484–88. <a href="https://doi.org/10.2996/kmj/1478073765" class="uri">https://doi.org/10.2996/kmj/1478073765</a>.</p>
</div>
<div id="ref-MNSV11">
<p>Majumdar, Satya N., Céline Nadal, Antonello Scardicchio, and Pierpaolo Vivo. 2011. “How Many Eigenvalues of a Gaussian Random Matrix Are Positive?” <em>Phys. Rev. E</em> 83 (4). American Physical Society: 041105. doi:<a href="https://doi.org/10.1103/PhysRevE.83.041105">10.1103/PhysRevE.83.041105</a>.</p>
</div>
<div id="ref-McCT14">
<p>McCoy, Michael B., and Joel A. Tropp. 2014. “From Steiner Formulas for Cones to Concentration of Intrinsic Volumes.” <em>Discrete Comput. Geom.</em> 51 (4): 926–63. doi:<a href="https://doi.org/10.1007/s00454-014-9595-4">10.1007/s00454-014-9595-4</a>.</p>
</div>
<div id="ref-NRS10">
<p>Nie, Jiawang, Kristian Ranestad, and Bernd Sturmfels. 2010. “The Algebraic Degree of Semidefinite Programming.” <em>Math. Program.</em> 122 (2, Ser. A): 379–405. doi:<a href="https://doi.org/10.1007/s10107-008-0253-6">10.1007/s10107-008-0253-6</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>In the original paper the authors use the parameter <span class="math inline">\(a=b+1\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>This statement depends on the type of convergence of the empirical distribution function. In fact, the paper <span class="citation">(Majumdar et al. <a href="#ref-MNSV11">2011</a>)</span> is a bit nonrigorous concerning these technical details. But the close connection between the empirical distribution function and the squared norms of the positive and negative parts allows for derivations of rigorous convergence results for these from similar results for the empirical distribution function. Also note that one can write the squared norm of the eigenvalues as the trace of the squared matrix, which yields another slightly different perspective, which in the unconstrained case is well understood. Again, the difference to the classical case lies in the constraints on the index and the replacement of the negative eigenvalues with zero.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#symm_cones">Symmetric cones</a></li>
      <li><a href="#curv_meas">Curvature measures of symmetric cones</a></li>
      <li><a href="#implem">Implementations</a></li>
      <li><a href="#look_ahead">Looking ahead</a></li>
      <li><a href="#asym_indnorm">Deriving the asymptotics for index constrained curvature measures</a></li>
      <li><a href="#appl_SDP">Application in semidefinite programming</a></li>
      <li><a href="#alg_deg">Appendix: Comparing curvature measures and algebraic degree</a></li>
      <li><a href="#refs">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Dennis Amelunxen.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
